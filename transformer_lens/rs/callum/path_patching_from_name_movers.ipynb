{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 30\n",
    "ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric_noising = generate_data_and_caches(N, model, verbose=True)\n",
    "seq_len = ioi_dataset.toks.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.tokenizer.pad_token_id, model.tokenizer.eos_token_id, model.tokenizer.bos_token_id"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Patch from neg NMHs to neg NMHs\n",
    "\n",
    "Theory was that the neg NMHs only suppress the IO token because the IO token gets predicted (thanks to the NMHs). In other words, they aren't acting in parallel to the NMHs while doing the opposite; rather they're taking the NMH output into their query vectors, using that to attend to the IO token, and then moving \"suppress IO prediction\" to the end token.\n",
    "\n",
    "We already know the OV circuit basically does negative copying for names, so the key hypothesis here is what goes into the query. The plot below shows that path patching from NMH output to NNMH query input does actually significantly affect the attention patterns: it turns them from from \"end token attends a lot more to IO than to S1\" to \"end token attends about equally to both\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEG_NAME_MOVERS = [(10, 7), (11, 10)]\n",
    "NAME_MOVERS = [(9, 6), (9, 9), (10, 10)]\n",
    "\n",
    "\n",
    "def get_io_vs_s_attn_for_nmh(\n",
    "    patched_cache: ActivationCache,\n",
    "    orig_dataset: IOIDataset,\n",
    "    orig_cache: ActivationCache,\n",
    "    neg_nmh: Tuple[int, int],\n",
    ") -> Float[Tensor, \"batch\"]:\n",
    "    '''\n",
    "    Returns the difference between patterns[END, IO] and patterns[END, S1], where patterns\n",
    "    are the attention patterns for the negative name mover head.\n",
    "\n",
    "    This is returned in the form of a tuple of 2 tensors: one for the patched distribution\n",
    "    (calculated using `patched_cache` which is returned by the path patching algorithm), and\n",
    "    one for the clean IOI distribution (which is just calculated directly from that cache).\n",
    "    '''\n",
    "    layer, head = neg_nmh\n",
    "    attn_pattern_patched = patched_cache[\"pattern\", layer][:, head]\n",
    "    attn_pattern_clean = orig_cache[\"pattern\", layer][:, head]\n",
    "    # both are (batch, seq_Q, seq_K), and I want all the \"end -> IO\" attention probs\n",
    "\n",
    "    N = orig_dataset.toks.size(0)\n",
    "    io_seq_pos = orig_dataset.word_idx[\"IO\"]\n",
    "    s1_seq_pos = orig_dataset.word_idx[\"S1\"]\n",
    "    end_seq_pos = orig_dataset.word_idx[\"end\"]\n",
    "\n",
    "    return (\n",
    "        attn_pattern_patched[range(N), end_seq_pos, io_seq_pos] - attn_pattern_patched[range(N), end_seq_pos, s1_seq_pos],\n",
    "        attn_pattern_clean[range(N), end_seq_pos, io_seq_pos] - attn_pattern_clean[range(N), end_seq_pos, s1_seq_pos],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_nnmh_patching_patterns(num_batches = 40, neg_nmh = NEG_NAME_MOVERS[0], orig_is_ioi = True):\n",
    "    results_patched = t.empty(size=(0,)).to(device)\n",
    "    results_clean = t.empty(size=(0,)).to(device)\n",
    "\n",
    "    for seed in tqdm(range(num_batches)):\n",
    "\n",
    "        ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric = generate_data_and_caches(20, model=model, seed=seed)\n",
    "\n",
    "        if orig_is_ioi:\n",
    "            orig_dataset = ioi_dataset\n",
    "            new_dataset = abc_dataset\n",
    "            orig_cache = ioi_cache\n",
    "            new_cache = abc_cache\n",
    "        else:\n",
    "            orig_dataset = abc_dataset\n",
    "            new_dataset = ioi_dataset\n",
    "            orig_cache = abc_cache\n",
    "            new_cache = ioi_cache\n",
    "\n",
    "        new_results_patched, new_results_clean = path_patch(\n",
    "            model,\n",
    "            orig_input=orig_dataset.toks,\n",
    "            new_input=new_dataset.toks,\n",
    "            orig_cache=orig_cache,\n",
    "            new_cache=new_cache,\n",
    "            sender_nodes=[Node(\"z\", layer=layer, head=head) for layer, head in NAME_MOVERS], # Output of all name mover heads\n",
    "            receiver_nodes=Node(\"q\", neg_nmh[0], head=neg_nmh[1]), # To query input of negative name mover head\n",
    "            patching_metric=partial(get_io_vs_s_attn_for_nmh, orig_dataset=orig_dataset, orig_cache=orig_cache, neg_nmh=neg_nmh),\n",
    "            apply_metric_to_cache=True,\n",
    "            direct_includes_mlps=not(model.cfg.use_split_qkv_input),\n",
    "        )\n",
    "        results_patched = t.concat([results_patched, new_results_patched])\n",
    "        results_clean = t.concat([results_clean, new_results_clean])\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    return results_patched, results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_patched, results_clean = get_nnmh_patching_patterns(neg_nmh = (10, 7))\n",
    "\n",
    "hist(\n",
    "    [results_patched, results_clean],\n",
    "    labels={\"variable\": \"Version\", \"value\": \"Attn diff (positive ⇒ more attn paid to IO than S1)\"},\n",
    "    title=\"Difference in attn from END➔IO vs. END➔S1 (path-patched vs clean)\",\n",
    "    names=[\"Patched (ABC)\", \"Clean (IOI)\"],\n",
    "    width=800,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_patched, results_clean = get_nnmh_patching_patterns(neg_nmh = (11, 10))\n",
    "\n",
    "hist(\n",
    "    [results_patched, results_clean],\n",
    "    labels={\"variable\": \"Version\", \"value\": \"Attn diff (positive ⇒ more attn paid to IO than S1)\"},\n",
    "    title=\"Difference in attn from END➔IO vs. END➔S1 (path-patched vs clean)\",\n",
    "    names=[\"Patched (ABC)\", \"Clean (IOI)\"],\n",
    "    width=800,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I just realised, this should have been obvious from the paper. We can see the neg name mover heads' output getting much less significant post-ablation. In fact, `10.7` is entirely wiped out.\n",
    "\n",
    "<img src=\"https://raw.githubusercontent.com/callummcdougall/computational-thread-art/master/example_images/misc/negnmh.png\" width=\"600\">"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Path patching from NMHs to backup NMHs\n",
    "\n",
    "What's the equivalent result we might expect when backup name mover heads are considered?\n",
    "\n",
    "It's the opposite - we expect them to not attend from END to IO much when the name mover heads are working normally, but when we path patch from name movers to backup name movers, that's when they'll kick in.\n",
    "\n",
    "I'll do the plot for `11.2`, `10.6`, and `10.10`, because these are the biggest backup heads. I'll try doing the patching in both directions (from IOI to ABC, and vice-versa)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_patched, results_clean = get_nnmh_patching_patterns(num_batches=50, neg_nmh=(10, 2))\n",
    "\n",
    "hist(\n",
    "    [results_patched, results_clean],\n",
    "    labels={\"variable\": \"Version\", \"value\": \"Attn diff (positive ⇒ more attn paid to IO than S1)\"},\n",
    "    title=\"Difference in attn from END➔IO vs. END➔S1 (path-patched vs clean)\",\n",
    "    names=[\"Patched (ABC)\", \"Orig (IOI)\"],\n",
    "    width=800,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_patched, results_clean = get_nnmh_patching_patterns(num_batches=50, neg_nmh=(10, 2), orig_is_ioi=False)\n",
    "\n",
    "hist(\n",
    "    [results_patched, results_clean],\n",
    "    labels={\"variable\": \"Version\", \"value\": \"Attn diff (positive ⇒ more attn paid to IO than S1)\"},\n",
    "    title=\"Difference in attn from END➔IO vs. END➔S1 (path-patched vs clean)\",\n",
    "    names=[\"Patched (IOI)\", \"Orig (ABC)\"],\n",
    "    width=800,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion - we do kinda see this (patching increases attn), but the effect is very weak."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
