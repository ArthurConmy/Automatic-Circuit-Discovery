{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "# model.set_use_split_qkv_input(True)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# full_data = get_webtext()\n",
    "# TOTAL_OWT_SAMPLES = 100\n",
    "# SEQ_LEN = 20\n",
    "# data = full_data[:TOTAL_OWT_SAMPLES]\n",
    "\n",
    "from transformer_lens import FactoredMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_embedding(model: HookedTransformer) -> Float[Tensor, \"d_vocab d_model\"]:\n",
    "\n",
    "    W_E = model.W_E.clone()\n",
    "    W_U = model.W_U.clone()\n",
    "    # t.testing.assert_close(W_E[:10, :10], W_U[:10, :10].T)  NOT TRUE, because of the center unembed part!\n",
    "\n",
    "    embeds = W_E.unsqueeze(0)\n",
    "    pre_attention = model.blocks[0].ln1(embeds)\n",
    "    post_attention = einops.einsum(\n",
    "        pre_attention, \n",
    "        model.W_V[0],\n",
    "        model.W_O[0],\n",
    "        \"b s d_model, num_heads d_model d_head, num_heads d_head d_model_out -> b s d_model_out\",\n",
    "    )\n",
    "    resid_mid = post_attention + embeds\n",
    "    normalized_resid_mid = model.blocks[0].ln2(resid_mid)\n",
    "    mlp_out = model.blocks[0].mlp(normalized_resid_mid)\n",
    "    \n",
    "    W_EE = mlp_out.squeeze()\n",
    "    W_EE_full = resid_mid.squeeze() + mlp_out.squeeze()\n",
    "\n",
    "    return {\n",
    "        \"W_U (or W_E, no MLPs)\": W_U.T,\n",
    "        # \"W_E (raw, no MLPs)\": W_E,\n",
    "        \"W_E (including MLPs)\": W_EE_full,\n",
    "        \"W_E (only MLPs)\": W_EE\n",
    "    }\n",
    "\n",
    "embeddings_dict = get_effective_embedding(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "W_E^Q W_Q W_K^T W_E^K\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_random_sample(\n",
    "    embeddings_dict: Dict[str, Float[Tensor, \"d_vocab d_model\"]],\n",
    "    model: HookedTransformer = model,\n",
    "    sample_size: int = 50,\n",
    "    num_batches: int = 1,\n",
    "    head: Tuple[int, int] = (10, 7)\n",
    "):\n",
    "    results_for_each_batch = []\n",
    "\n",
    "    sorted_keys = sorted(embeddings_dict.keys())\n",
    "\n",
    "    W_Q = model.W_Q[head[0], head[1]]\n",
    "    W_K = model.W_K[head[0], head[1]]\n",
    "\n",
    "    embeddings_dict_normalized = {k: v / v.var(dim=-1, keepdim=True).pow(0.5) for k, v in embeddings_dict.items()}\n",
    "\n",
    "    q_and_k_labels = [(q_name, k_name) for q_name in sorted_keys for k_name in sorted_keys]\n",
    "    q_and_k_matrices = [(embeddings_dict_normalized[q_name], embeddings_dict_normalized[k_name]) for (q_name, k_name) in q_and_k_labels]\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        results = []\n",
    "        sample_indices = t.randint(0, model.cfg.d_vocab, (sample_size,))\n",
    "        for q_matrix, k_matrix in q_and_k_matrices:\n",
    "            full_matrix = FactoredMatrix(q_matrix @ W_Q, W_K.T @ k_matrix.T)\n",
    "            full_matrix_sample = full_matrix.A[sample_indices, :] @ full_matrix.B[:, sample_indices]\n",
    "            # full_matrix_sample = full_matrix_sample - full_matrix_sample.mean(dim=-1, keepdim=True)\n",
    "            # full_matrix_sample = full_matrix_sample / full_matrix_sample.std()\n",
    "            full_matrix_sample = full_matrix_sample.softmax(dim=-1)\n",
    "            results.append(full_matrix_sample)\n",
    "\n",
    "        results_for_each_batch.append(t.stack(results, dim=0))\n",
    "\n",
    "    results = sum(results_for_each_batch) / len(results_for_each_batch)\n",
    "\n",
    "    imshow(\n",
    "        results,\n",
    "        facet_col=0,\n",
    "        facet_col_wrap=len(embeddings_dict),\n",
    "        facet_labels=[f\"Q = {q_name}<br>K = {k_name}\" for (q_name, k_name) in q_and_k_labels],\n",
    "        title=f\"Sample of diagonal patterns for differnet matrices: head {head}\",\n",
    "        labels={\"x\": \"Key\", \"y\": \"Query\"},\n",
    "        height=900, width=900\n",
    "    )\n",
    "    results_trace = results[:, range(sample_size), range(sample_size)].mean(-1).reshape((len(sorted_keys), len(sorted_keys)))\n",
    "    imshow(\n",
    "        1 / (1 - results_trace),\n",
    "        x = sorted_keys,\n",
    "        y = sorted_keys,\n",
    "        title=f\"1 / (1 - avg_trace) for {head} (to make close to one blow up!)\",\n",
    "        labels={\"x\": \"Key\", \"y\": \"Query\"},\n",
    "        height=500, width=600\n",
    "    )\n",
    "\n",
    "plot_random_sample(embeddings_dict, head = (11, 8), sample_size = 100, num_batches = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores_for_all_heads(\n",
    "    embeddings_dict: Dict[str, Float[Tensor, \"d_vocab d_model\"]],\n",
    "    model: HookedTransformer = model,\n",
    "    sample_size: int = 50,\n",
    "    num_batches: int = 1,\n",
    "    include_W_E_raw: bool = True,\n",
    "    plot_probs: bool = False,\n",
    "):\n",
    "    results = []\n",
    "\n",
    "    sorted_keys = sorted(embeddings_dict.keys())\n",
    "\n",
    "    W_Qs = model.W_Q\n",
    "    W_Ks = model.W_K\n",
    "\n",
    "    W_U_Q = embeddings_dict[\"W_U (or W_E, no MLPs)\"]\n",
    "    W_U_Q_normed = W_U_Q / W_U_Q.var(dim=-1, keepdim=True).pow(0.5)\n",
    "    if include_W_E_raw:\n",
    "        W_E_K = embeddings_dict[\"W_E (including MLPs)\"]\n",
    "    else:\n",
    "        W_E_K = embeddings_dict[\"W_E (only MLPs)\"]\n",
    "    W_E_K_normed = W_E_K / W_E_K.var(dim=-1, keepdim=True).pow(0.5)\n",
    "\n",
    "    W_Q_full = einops.einsum(W_U_Q_normed, W_Qs, \"d_vocab d_model, layer head d_model d_head -> layer head d_vocab d_head\")\n",
    "    W_K_full = einops.einsum(W_E_K_normed, W_Ks, \"d_vocab d_model, layer head d_model d_head -> layer head d_vocab d_head\")\n",
    "\n",
    "    W_QK_full = FactoredMatrix(W_Q_full, W_K_full.transpose(-1, -2))\n",
    "\n",
    "    for _ in range(num_batches):\n",
    "        sample_indices = t.randint(0, model.cfg.d_vocab, (sample_size,))\n",
    "        W_QK_sample = W_QK_full.A[..., sample_indices, :] @ W_QK_full.B[..., :, sample_indices]\n",
    "        W_QK_sample = W_QK_sample - W_QK_sample.mean(dim=-1, keepdim=True)\n",
    "\n",
    "        if plot_probs:\n",
    "            W_QK_softmaxed = W_QK_sample.softmax(dim=-1)\n",
    "            W_QK_avg_diag_prob = W_QK_softmaxed[..., range(sample_size), range(sample_size)].mean(-1)\n",
    "            results.append(W_QK_avg_diag_prob)\n",
    "        else:\n",
    "            W_QK_diag_sum = W_QK_sample[..., range(sample_size), range(sample_size)].sum(-1)\n",
    "            W_QK_offdiag_sum = W_QK_sample.sum(dim=(-1, -2)) - W_QK_diag_sum\n",
    "            W_QK_avg_diag = W_QK_diag_sum / sample_size\n",
    "            W_QK_avg_offdiag = W_QK_offdiag_sum / sample_size\n",
    "            results.append(W_QK_avg_diag - W_QK_avg_offdiag)\n",
    "\n",
    "\n",
    "    results = sum(results) / len(results)\n",
    "    return results[1:]\n",
    "\n",
    "\n",
    "results = get_scores_for_all_heads(embeddings_dict, sample_size = 250, num_batches = 40, plot_probs = True)\n",
    "\n",
    "imshow(results, y=list(range(1, 12)), labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attention scores (prob space, including MLP & W_E)\", width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = get_scores_for_all_heads(embeddings_dict, sample_size = 200, num_batches = 40, include_W_E_raw = False, plot_probs = True)\n",
    "\n",
    "imshow(results, y=list(range(1, 12)), labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attention scores (prob space, including MLP & not W_E)\", width=600)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
