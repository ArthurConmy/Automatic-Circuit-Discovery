{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_str(s: str):\n",
    "    doubles = \"“”\"\n",
    "    singles = \"‘’\"\n",
    "    for char in doubles: s = s.replace(char, '\"')\n",
    "    for char in singles: s = s.replace(char, \"'\")\n",
    "    return s\n",
    "\n",
    "def parse_str_tok_for_printing(s: str):\n",
    "    s = s.replace(\"\\n\", \"\\\\n\")\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 10\n",
    "SEQ_LEN = 100 # 1024\n",
    "\n",
    "DATA_STR = get_webtext(seed=6)[:BATCH_SIZE]\n",
    "DATA_STR = [parse_str(s) for s in DATA_STR]\n",
    "\n",
    "DATA_TOKS = model.to_tokens(DATA_STR)\n",
    "DATA_STR_TOKS = model.to_str_tokens(DATA_STR)\n",
    "\n",
    "if SEQ_LEN < 1024:\n",
    "    DATA_TOKS = DATA_TOKS[:, :SEQ_LEN]\n",
    "    DATA_STR_TOKS = [str_toks[:SEQ_LEN] for str_toks in DATA_STR_TOKS]\n",
    "\n",
    "DATA_STR_TOKS_PARSED = [[parse_str_tok_for_printing(tok) for tok in toks] for toks in DATA_STR_TOKS]\n",
    "\n",
    "clear_output()\n",
    "\n",
    "print(DATA_TOKS.shape, \"\\n\")\n",
    "\n",
    "print(DATA_STR_TOKS[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data\n",
    "\n",
    "Here's where I gather data for the other visualisations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field\n",
    "from typing import Any\n",
    "Head = Tuple[int, int]\n",
    "\n",
    "class HeadResults:\n",
    "    data: Dict[Head, Tensor]\n",
    "    def __init__(self, data=None):\n",
    "        if data is None: # ! bad practice to have default arguments be dicts\n",
    "            data = {}\n",
    "        self.data = data\n",
    "\n",
    "    def __getitem__(self, layer_and_head) -> Tensor:\n",
    "        return self.data[layer_and_head].clone()\n",
    "    \n",
    "    def __setitem__(self, layer_and_head, value):\n",
    "        self.data[layer_and_head] = value.clone()\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class LogitResults:\n",
    "    zero_patched: HeadResults = HeadResults()\n",
    "    mean_patched: HeadResults = HeadResults()\n",
    "    zero_direct: HeadResults = HeadResults()\n",
    "    mean_direct: HeadResults = HeadResults()\n",
    "\n",
    "@dataclass(frozen=False)\n",
    "class ModelResults:\n",
    "    logits_orig: Tensor = t.empty(0)\n",
    "    loss_orig: Tensor = t.empty(0)\n",
    "    result: HeadResults = HeadResults()\n",
    "    result_mean: HeadResults = HeadResults()\n",
    "    pattern: HeadResults = HeadResults()\n",
    "    direct_effect: HeadResults = HeadResults()\n",
    "    direct_effect_mean: HeadResults = HeadResults()\n",
    "    scale: Tensor = t.empty(0)\n",
    "    logits: LogitResults = LogitResults()\n",
    "    loss: LogitResults = LogitResults()\n",
    "\n",
    "    def clear(self):\n",
    "        # Empties all intermediate results which we don't need\n",
    "        self.result = HeadResults()\n",
    "        self.result_mean = HeadResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_dict(\n",
    "    model: HookedTransformer,\n",
    "    toks: Int[Tensor, \"batch seq\"],\n",
    "    negative_heads: List[Tuple[int, int]],\n",
    "    use_cuda: bool = False,\n",
    "):\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "    device = str(model.cfg.device)\n",
    "    if use_cuda: model = model.cuda()\n",
    "    else: model = model.cpu()\n",
    "\n",
    "    model_results = ModelResults()\n",
    "\n",
    "    # Cache the head results and attention patterns, and final ln scale\n",
    "\n",
    "    def cache_head_result(result: Float[Tensor, \"batch seq n_heads d_model\"], hook: HookPoint, head: int):\n",
    "        model_results.result[hook.layer(), head] = result[:, :, head]\n",
    "    \n",
    "    def cache_head_pattern(pattern: Float[Tensor, \"batch n_heads seq_Q seq_K\"], hook: HookPoint, head: int):\n",
    "        model_results.pattern[hook.layer(), head] = pattern[:, head]\n",
    "    \n",
    "    def cache_scale(scale: Float[Tensor, \"batch seq 1\"], hook: HookPoint):\n",
    "        model_results.scale = scale\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "        model.add_hook(utils.get_act_name(\"result\", layer), partial(cache_head_result, head=head))\n",
    "        model.add_hook(utils.get_act_name(\"pattern\", layer), partial(cache_head_pattern, head=head))\n",
    "    model.add_hook(utils.get_act_name(\"scale\"), cache_scale)\n",
    "\n",
    "    # Run the forward pass, to cache all values (and get logits)\n",
    "\n",
    "    model_results.logits_orig, model_results.loss_orig = model(toks, return_type=\"both\", loss_per_token=True)\n",
    "\n",
    "    # Calculate the thing we'll be subbing in for mean ablation\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "        model_results.result_mean[layer, head] = einops.reduce(\n",
    "            model_results.result[layer, head], \n",
    "            \"batch seq d_model -> d_model\", \"mean\"\n",
    "        )\n",
    "\n",
    "    # Now, use \"result\" to get the thing we'll eventually be adding to logits (i.e. scale it and map it through W_U)\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "\n",
    "        model_results.direct_effect[layer, head] = einops.einsum(\n",
    "            model_results.result[layer, head] / model_results.scale,\n",
    "            model.W_U,\n",
    "            \"batch seq d_model, d_model d_vocab -> batch seq d_vocab\"\n",
    "        )\n",
    "        model_results.direct_effect_mean[layer, head] = einops.reduce(\n",
    "            model_results.direct_effect[layer, head],\n",
    "            \"batch seq d_vocab -> d_vocab\",\n",
    "            \"mean\"\n",
    "        )\n",
    "\n",
    "    # Two new forward passes: one with mean ablation, one with zero ablation. We only store logits from these\n",
    "\n",
    "    def patch_head_result(\n",
    "        result: Float[Tensor, \"batch seq n_heads d_model\"],\n",
    "        hook: HookPoint,\n",
    "        head: int,\n",
    "        ablation_values: Optional[HeadResults] = None,\n",
    "    ):\n",
    "        if ablation_values is None:\n",
    "            result[:, :, head] = t.zeros_like(result[:, :, head])\n",
    "        else:\n",
    "            result[:, :, head] = ablation_values[hook.layer(), head]\n",
    "        return result\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "        model.add_hook(utils.get_act_name(\"result\", layer), partial(patch_head_result, head=head))\n",
    "        model_results.logits.zero_patched[layer, head] = model(toks, return_type=\"logits\")\n",
    "        model.add_hook(utils.get_act_name(\"result\", layer), partial(patch_head_result, head=head, ablation_values=model_results.result_mean))\n",
    "        model_results.logits.mean_patched[layer, head] = model(toks, return_type=\"logits\")\n",
    "    \n",
    "    model_results.clear()\n",
    "\n",
    "    # Now, the direct effects\n",
    "\n",
    "    for layer, head in negative_heads:\n",
    "        # Get the change in logits from removing the direct effect of the head\n",
    "        model_results.logits.zero_direct[layer, head] = model_results.logits_orig - model_results.direct_effect[layer, head]\n",
    "        # Get the change in logits from removing the direct effect of the head, and replacing with the mean effect\n",
    "        model_results.logits.mean_direct[layer, head] = model_results.logits.zero_direct[layer, head] + model_results.direct_effect_mean[layer, head]\n",
    "\n",
    "    # Calculate the loss for all of these\n",
    "    for k in [\"zero_patched\", \"mean_patched\", \"zero_direct\", \"mean_direct\"]:\n",
    "        setattr(model_results.loss, k, HeadResults({\n",
    "            (layer, head): model.loss_fn(getattr(model_results.logits, k)[layer, head], toks, per_token=True)\n",
    "            for layer, head in negative_heads\n",
    "        }))\n",
    "\n",
    "    model = model.to(device)\n",
    "    return model_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_RESULTS = get_data_dict(model, DATA_TOKS, negative_heads = [(10, 7), (11, 10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activations\n",
    "\n",
    "Here's where I get the activation plots, where each value actually shows the effect on logits of ablating.\n",
    "\n",
    "We show `(original loss) - (ablated loss)`, so red (negativity) indicates this head makes performance worse. The examples where this head is most valuable are the very positive examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert MODEL_RESULTS.loss_orig.shape == MODEL_RESULTS.loss.mean_patched[(10, 7)].shape == (BATCH_SIZE, SEQ_LEN - 1)\n",
    "\n",
    "loss_diffs = MODEL_RESULTS.loss_orig - t.stack([\n",
    "    MODEL_RESULTS.loss.mean_patched[(10, 7)],\n",
    "    MODEL_RESULTS.loss.zero_patched[(10, 7)],\n",
    "    MODEL_RESULTS.loss.mean_direct[(10, 7)],\n",
    "    MODEL_RESULTS.loss.zero_direct[(10, 7)],\n",
    "])\n",
    "loss_diffs_padded = t.concat([loss_diffs, t.zeros((4, BATCH_SIZE, 1))], dim=-1)\n",
    "loss_diffs_padded = list(einops.rearrange(\n",
    "    loss_diffs_padded, \"loss_type batch seq -> batch seq loss_type\"\n",
    ").unsqueeze(-1).unbind(0))\n",
    "\n",
    "cv.activations.text_neuron_activations(\n",
    "    tokens = DATA_STR_TOKS_PARSED,\n",
    "    activations = loss_diffs_padded,\n",
    "    first_dimension_name = \"loss_type\",\n",
    "    first_dimension_labels = [\"mean, patched\", \"zero, patched\", \"mean, direct\", \"zero, direct\"],\n",
    "    second_dimension_name = \"(ignore)\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
