{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "from transformer_lens.rs.callum.generate_bag_of_words_quad_plot import get_effective_embedding, lock_attn, fwd_pass_lock_attn0_to_self, get_EE_QK_circuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-small\"\n",
    "# MODEL_NAME = \"solu-10l\"\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    MODEL_NAME,\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "\n",
    "model.set_use_attn_result(False)\n",
    "model.set_use_split_qkv_input(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX, HEAD_IDX = {\n",
    "    \"SoLU_10L1280W_C4_Code\": (9, 18), # (9, 18) is somewhat cheaty\n",
    "    \"gpt2\": (10, 7),\n",
    "}[model.cfg.model_name]\n",
    "\n",
    "\n",
    "W_U = model.W_U\n",
    "W_Q_negative = model.W_Q[LAYER_IDX, HEAD_IDX]\n",
    "W_K_negative = model.W_K[LAYER_IDX, HEAD_IDX]\n",
    "\n",
    "W_E = model.W_E\n",
    "\n",
    "# ! question - what's the approximation of GPT2-small's embedding?\n",
    "# lock attn to 1 at current position\n",
    "# lock attn to average\n",
    "# don't include attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_QK_circuit = FactoredMatrix(W_U.T @ W_Q_negative, W_K_negative.T @ W_E.T)\n",
    "\n",
    "indices = t.randint(0, model.cfg.d_vocab, (250,))\n",
    "full_QK_circuit_sample = full_QK_circuit.A[indices, :] @ full_QK_circuit.B[:, indices]\n",
    "\n",
    "full_QK_circuit_sample_centered = full_QK_circuit_sample - full_QK_circuit_sample.mean(dim=1, keepdim=True)\n",
    "\n",
    "imshow(\n",
    "    full_QK_circuit_sample_centered,\n",
    "    labels={\"x\": \"Source / key token (embedding)\", \"y\": \"Destination / query token (unembedding)\"},\n",
    "    title=\"Full QK circuit for negative name mover head\",\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"stas/openwebtext-10k\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "dataset = [train_dataset[i][\"text\"] for i in range(len(train_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(dataset):\n",
    "    loss_hooked = fwd_pass_lock_attn0_to_self(model, s)\n",
    "    print(f\"Loss with attn locked to self: {loss_hooked:.2f}\")\n",
    "    loss_hooked_0 = fwd_pass_lock_attn0_to_self(model, s, ablate=True)\n",
    "    print(f\"Loss with attn locked to zero: {loss_hooked_0:.2f}\")\n",
    "    loss_orig = model(s, return_type=\"loss\")\n",
    "    print(f\"Loss with attn free: {loss_orig:.2f}\\n\")\n",
    "\n",
    "    # gc.collect()\n",
    "\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gpt\" in model.cfg.model_name: # sigh, tied embeddings\n",
    "    # sanity check this is the same \n",
    "\n",
    "    def remove_pos_embed(z, hook):\n",
    "        return 0.0 * z\n",
    "\n",
    "    # setup a forward pass that \n",
    "    model.reset_hooks()\n",
    "    model.add_hook(\n",
    "        name=\"hook_pos_embed\",\n",
    "        hook=remove_pos_embed,\n",
    "        level=1, # ???\n",
    "    ) \n",
    "    model.add_hook(\n",
    "        name=utils.get_act_name(\"pattern\", 0),\n",
    "        hook=lock_attn,\n",
    "    )\n",
    "    logits, cache = model.run_with_cache(\n",
    "        torch.arange(1000).to(device).unsqueeze(0),\n",
    "        names_filter=lambda name: name==\"blocks.1.hook_resid_pre\",\n",
    "        return_type=\"logits\",\n",
    "    )\n",
    "\n",
    "\n",
    "    W_EE_test = cache[\"blocks.1.hook_resid_pre\"].squeeze(0)\n",
    "    W_EE_prefix = W_EE_test[:1000]\n",
    "\n",
    "    assert torch.allclose(\n",
    "        W_EE_prefix,\n",
    "        W_EE_test,\n",
    "        atol=1e-4,\n",
    "        rtol=1e-4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MOVERS = {\n",
    "    \"gpt2\": [(9, 9), (10, 0), (9, 6)],\n",
    "    \"SoLU_10L1280W_C4_Code\": [(7, 12), (5, 4), (8, 3)],\n",
    "}[model.cfg.model_name]\n",
    "\n",
    "NEGATIVE_NAME_MOVERS = {\n",
    "    \"gpt2\": [(LAYER_IDX, HEAD_IDX), (11, 10)],\n",
    "    \"SoLU_10L1280W_C4_Code\": [(LAYER_IDX, HEAD_IDX), (9, 15)], # second one on this one IOI prompt only...\n",
    "}[model.cfg.model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep some bags of words...\n",
    "# OVERLY LONG because it really helps to have the bags of words the same length\n",
    "\n",
    "bags_of_words = []\n",
    "\n",
    "OUTER_LEN = 50\n",
    "INNER_LEN = 100\n",
    "\n",
    "idx = -1\n",
    "while len(bags_of_words) < OUTER_LEN:\n",
    "    idx += 1\n",
    "    cur_tokens = model.tokenizer.encode(dataset[idx])\n",
    "    cur_bag = []\n",
    "    \n",
    "    for i in range(len(cur_tokens)):\n",
    "        if len(cur_bag) == INNER_LEN:\n",
    "            break\n",
    "        if cur_tokens[i] not in cur_bag:\n",
    "            cur_bag.append(cur_tokens[i])\n",
    "\n",
    "    if len(cur_bag) == INNER_LEN:\n",
    "        bags_of_words.append(cur_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_dict = get_effective_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting just diag patterns for a single head\n",
    "\n",
    "from transformer_lens import FactoredMatrix\n",
    "\n",
    "LAYER = 3\n",
    "HEAD = 0\n",
    "\n",
    "all_results = []\n",
    "embeddings_dict_keys = sorted(embeddings_dict.keys())\n",
    "labels = []\n",
    "\n",
    "for q_side_matrix, k_side_matrix in tqdm(list(itertools.product(embeddings_dict_keys, embeddings_dict_keys))):\n",
    "    labels.append(f\"Q = {q_side_matrix}<br>K = {k_side_matrix}\")\n",
    "\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            LAYER,\n",
    "            HEAD,\n",
    "            model,\n",
    "            show_plot=False,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx: idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[q_side_matrix],\n",
    "            W_E_key_side=embeddings_dict[k_side_matrix],\n",
    "        )\n",
    "        results.append(softmaxed_attn)\n",
    "    \n",
    "    all_results.append(sum(results) / len(results))\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "all_results = t.stack(all_results) # .reshape((3, 3, INNER_LEN, INNER_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    all_results,\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=len(embeddings_dict),\n",
    "    facet_labels=labels,\n",
    "    title=f\"Sample of diagonal patterns for different matrices: head 3.0 (duplicate token head)\",\n",
    "    labels={\"x\": \"Key\", \"y\": \"Query\"},\n",
    "    height=900, width=900\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = t.zeros(12, 12).float().to(device)\n",
    "\n",
    "for layer, head in tqdm(list(itertools.product(range(12), range(12)))):\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            layer,\n",
    "            head,\n",
    "            model,\n",
    "            show_plot=False,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx:idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[\"W_U (or W_E, no MLPs)\"],\n",
    "            W_E_key_side=embeddings_dict[\"W_E (including MLPs)\"],  # \"W_E (only MLPs)\"\n",
    "        )\n",
    "        results.append(softmaxed_attn.diag().mean())\n",
    "\n",
    "    results = sum(results) / len(results)\n",
    "\n",
    "    scores[layer, head] = results\n",
    "\n",
    "imshow(scores, width=750, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attn scores for bag of words (including MLPs in embedding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = t.zeros(12, 12).float().to(device)\n",
    "\n",
    "for layer, head in tqdm(list(itertools.product(range(12), range(12)))):\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            layer,\n",
    "            head,\n",
    "            model,\n",
    "            show_plot=False,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx:idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[\"W_U (or W_E, no MLPs)\"],\n",
    "            W_E_key_side=embeddings_dict[\"W_E (only MLPs)\"],  # \n",
    "        )\n",
    "        results.append(softmaxed_attn.diag().mean())\n",
    "\n",
    "    results = sum(results) / len(results)\n",
    "\n",
    "    scores[layer, head] = results\n",
    "\n",
    "imshow(scores, width=750, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attn scores for bag of words (only MLPs in embedding)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
