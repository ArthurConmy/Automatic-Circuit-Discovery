{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"ACCELERATE_DISABLE_RICH\"] = \"1\"\n",
    "from typeguard import typechecked\n",
    "from transformer_lens import HookedTransformer\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from IPython import get_ipython\n",
    "ipython = get_ipython()\n",
    "ipython.run_line_magic(\"load_ext\", \"autoreload\")\n",
    "ipython.run_line_magic(\"autoreload\", \"2\")\n",
    "\n",
    "import torch as t\n",
    "import torch\n",
    "import einops\n",
    "import itertools\n",
    "import plotly.express as px\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from jaxtyping import Float, Int, jaxtyped\n",
    "from typing import Union, List, Dict, Tuple, Callable, Optional\n",
    "from torch import Tensor\n",
    "import gc\n",
    "import transformer_lens\n",
    "from transformer_lens.ActivationCache import ActivationCache\n",
    "from transformer_lens import utils\n",
    "from transformer_lens.utils import to_numpy\n",
    "t.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_tensor(\n",
    "    tensor,\n",
    "):\n",
    "    return t.from_numpy(to_numpy(tensor))\n",
    "\n",
    "def imshow_old(\n",
    "    tensor, \n",
    "    **kwargs,\n",
    "):\n",
    "    tensor = to_tensor(tensor)\n",
    "    zmax = tensor.abs().max().item()\n",
    "\n",
    "    if \"zmin\" not in kwargs:\n",
    "        kwargs[\"zmin\"] = -zmax\n",
    "    if \"zmax\" not in kwargs:\n",
    "        kwargs[\"zmax\"] = zmax\n",
    "    if \"color_continuous_scale\" not in kwargs:\n",
    "        kwargs[\"color_continuous_scale\"] = \"RdBu\"\n",
    "\n",
    "    fig = px.imshow(\n",
    "        to_numpy(tensor),\n",
    "        **kwargs,\n",
    "    )\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = \"gpt2-small\"\n",
    "# MODEL_NAME = \"solu-10l\"\n",
    "model = transformer_lens.HookedTransformer.from_pretrained(MODEL_NAME)\n",
    "from transformer_lens.hackathon.ioi_dataset import IOIDataset, NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.set_use_attn_result(False)\n",
    "model.set_use_split_qkv_input(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LAYER_IDX, HEAD_IDX = {\n",
    "    \"SoLU_10L1280W_C4_Code\": (9, 18), # (9, 18) is somewhat cheaty\n",
    "    \"gpt2\": (10, 7),\n",
    "}[model.cfg.model_name]\n",
    "\n",
    "\n",
    "W_U = model.W_U\n",
    "W_Q_negative = model.W_Q[LAYER_IDX, HEAD_IDX]\n",
    "W_K_negative = model.W_K[LAYER_IDX, HEAD_IDX]\n",
    "\n",
    "W_E = model.W_E\n",
    "\n",
    "# ! question - what's the approximation of GPT2-small's embedding?\n",
    "# lock attn to 1 at current position\n",
    "# lock attn to average\n",
    "# don't include attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix\n",
    "\n",
    "full_QK_circuit = FactoredMatrix(W_U.T @ W_Q_negative, W_K_negative.T @ W_E.T)\n",
    "\n",
    "indices = t.randint(0, model.cfg.d_vocab, (250,))\n",
    "full_QK_circuit_sample = full_QK_circuit.A[indices, :] @ full_QK_circuit.B[:, indices]\n",
    "\n",
    "full_QK_circuit_sample_centered = full_QK_circuit_sample - full_QK_circuit_sample.mean(dim=1, keepdim=True)\n",
    "\n",
    "imshow(\n",
    "    full_QK_circuit_sample_centered,\n",
    "    labels={\"x\": \"Source / key token (embedding)\", \"y\": \"Destination / query token (unembedding)\"},\n",
    "    title=\"Full QK circuit for negative name mover head\",\n",
    "    width=700,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lock_attn(\n",
    "    attn_patterns: Float[t.Tensor, \"batch head_idx dest_pos src_pos\"],\n",
    "    hook: HookPoint,\n",
    "    ablate: bool = False,\n",
    ") -> Float[t.Tensor, \"batch head_idx dest_pos src_pos\"]:\n",
    "    \n",
    "    assert isinstance(attn_patterns, Float[t.Tensor, \"batch head_idx dest_pos src_pos\"])\n",
    "    assert hook.layer() == 0\n",
    "\n",
    "    batch, n_heads, seq_len = attn_patterns.shape[:3]\n",
    "    attn_new = einops.repeat(t.eye(seq_len), \"dest src -> batch head_idx dest src\", batch=batch, head_idx=n_heads).clone().to(attn_patterns.device)\n",
    "    if ablate:\n",
    "        attn_new = attn_new * 0\n",
    "    return attn_new\n",
    "\n",
    "def fwd_pass_lock_attn0_to_self(\n",
    "    model: HookedTransformer,\n",
    "    input: Union[List[str], Int[t.Tensor, \"batch seq_pos\"]],\n",
    "    ablate: bool = False,\n",
    ") -> Float[t.Tensor, \"batch seq_pos d_vocab\"]:\n",
    "\n",
    "    model.reset_hooks()\n",
    "    \n",
    "    loss = model.run_with_hooks(\n",
    "        input,\n",
    "        return_type=\"loss\",\n",
    "        fwd_hooks=[(utils.get_act_name(\"pattern\", 0), partial(lock_attn, ablate=ablate))],\n",
    "    )\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dataset = load_dataset(\"stas/openwebtext-10k\")\n",
    "train_dataset = raw_dataset[\"train\"]\n",
    "dataset = [train_dataset[i][\"text\"] for i in range(len(train_dataset))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, s in enumerate(dataset):\n",
    "    loss_hooked = fwd_pass_lock_attn0_to_self(model, s)\n",
    "    print(f\"Loss with attn locked to self: {loss_hooked:.2f}\")\n",
    "    loss_hooked_0 = fwd_pass_lock_attn0_to_self(model, s, ablate=True)\n",
    "    print(f\"Loss with attn locked to zero: {loss_hooked_0:.2f}\")\n",
    "    loss_orig = model(s, return_type=\"loss\")\n",
    "    print(f\"Loss with attn free: {loss_orig:.2f}\\n\")\n",
    "\n",
    "    # gc.collect()\n",
    "\n",
    "    if i == 5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"gpt\" in model.cfg.model_name: # sigh, tied embeddings\n",
    "    # sanity check this is the same \n",
    "\n",
    "    def remove_pos_embed(z, hook):\n",
    "        return 0.0 * z\n",
    "\n",
    "    # setup a forward pass that \n",
    "    model.reset_hooks()\n",
    "    model.add_hook(\n",
    "        name=\"hook_pos_embed\",\n",
    "        hook=remove_pos_embed,\n",
    "        level=1, # ???\n",
    "    ) \n",
    "    model.add_hook(\n",
    "        name=utils.get_act_name(\"pattern\", 0),\n",
    "        hook=lock_attn,\n",
    "    )\n",
    "    logits, cache = model.run_with_cache(\n",
    "        torch.arange(1000).to(device).unsqueeze(0),\n",
    "        names_filter=lambda name: name==\"blocks.1.hook_resid_pre\",\n",
    "        return_type=\"logits\",\n",
    "    )\n",
    "\n",
    "\n",
    "    W_EE_test = cache[\"blocks.1.hook_resid_pre\"].squeeze(0)\n",
    "    W_EE_prefix = W_EE_test[:1000]\n",
    "\n",
    "    assert torch.allclose(\n",
    "        W_EE_prefix,\n",
    "        W_EE_test,\n",
    "        atol=1e-4,\n",
    "        rtol=1e-4,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_EE_QK_circuit(\n",
    "    layer_idx,\n",
    "    head_idx,\n",
    "    random_seeds: Optional[int] = 5,\n",
    "    num_samples: Optional[int] = 500,\n",
    "    bags_of_words: Optional[List[List[int]]] = None, # each List is a List of unique tokens\n",
    "    mean_version: bool = True,\n",
    "    show_plot: bool = False,\n",
    "    W_E_query_side: Optional[t.Tensor] = None,\n",
    "    W_E_key_side: Optional[t.Tensor] = None,\n",
    "):\n",
    "    assert (random_seeds is None and num_samples is None) != (bags_of_words is None), (random_seeds is None, num_samples is None, bags_of_words is None, \"Must specify either random_seeds and num_samples or bag_of_words_version\")\n",
    "\n",
    "    if bags_of_words is not None:\n",
    "        random_seeds = len(bags_of_words) # eh not quite random seeds but whatever\n",
    "        assert all([len(bag_of_words) == len(bags_of_words[0])] for bag_of_words in bags_of_words), \"Must have same number of words in each bag of words\"\n",
    "        num_samples = len(bags_of_words[0])\n",
    "\n",
    "    W_Q_head = model.W_Q[layer_idx, head_idx]\n",
    "    W_K_head = model.W_K[layer_idx, head_idx]\n",
    "\n",
    "    assert W_E_query_side is not None\n",
    "    assert W_E_key_side is not None\n",
    "    W_E_Q_normed = W_E_query_side / W_E_query_side.var(dim=-1, keepdim=True).pow(0.5)\n",
    "    W_E_K_normed = W_E_key_side / W_E_key_side.var(dim=-1, keepdim=True).pow(0.5)\n",
    "\n",
    "    EE_QK_circuit = FactoredMatrix(W_E_Q_normed @ W_Q_head, W_K_head.T @ W_E_K_normed.T)\n",
    "    EE_QK_circuit_result = t.zeros((num_samples, num_samples))\n",
    "\n",
    "    for random_seed in range(random_seeds):\n",
    "        if bags_of_words is None:\n",
    "            indices = t.randint(0, model.cfg.d_vocab, (num_samples,))\n",
    "        else:\n",
    "            indices = t.tensor(bags_of_words[random_seed])\n",
    "\n",
    "        # assert False, \"TODO: add Q and K and V biases???\"\n",
    "        EE_QK_circuit_sample = einops.einsum(\n",
    "            EE_QK_circuit.A[indices, :],\n",
    "            EE_QK_circuit.B[:, indices],\n",
    "            \"num_query_samples d_head, d_head num_key_samples -> num_query_samples num_key_samples\"\n",
    "        )\n",
    "\n",
    "        if mean_version:\n",
    "            # we're going to take a softmax so the constant factor is arbitrary \n",
    "            # and it's a good idea to centre all these results so adding them up is reasonable\n",
    "            EE_QK_mean = EE_QK_circuit_sample.mean(dim=-1, keepdim=True)\n",
    "            EE_QK_circuit_sample_centered = EE_QK_circuit_sample - EE_QK_mean \n",
    "            EE_QK_circuit_result += EE_QK_circuit_sample_centered.cpu()\n",
    "\n",
    "        else:\n",
    "            EE_QK_softmax = t.nn.functional.softmax(EE_QK_circuit_sample, dim=-1)\n",
    "            EE_QK_circuit_result += EE_QK_softmax.cpu()\n",
    "\n",
    "    EE_QK_circuit_result /= random_seeds\n",
    "\n",
    "    if show_plot:\n",
    "        imshow_old(\n",
    "            EE_QK_circuit_result,\n",
    "            labels={\"x\": \"Source/Key Token (embedding)\", \"y\": \"Destination/Query Token (unembedding)\"},\n",
    "            title=f\"EE QK circuit for head {layer_idx}.{head_idx}\",\n",
    "            width=700,\n",
    "        )\n",
    "\n",
    "    return EE_QK_circuit_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MOVERS = {\n",
    "    \"gpt2\": [(9, 9), (10, 0), (9, 6)],\n",
    "    \"SoLU_10L1280W_C4_Code\": [(7, 12), (5, 4), (8, 3)],\n",
    "}[model.cfg.model_name]\n",
    "\n",
    "NEGATIVE_NAME_MOVERS = {\n",
    "    \"gpt2\": [(LAYER_IDX, HEAD_IDX), (11, 10)],\n",
    "    \"SoLU_10L1280W_C4_Code\": [(LAYER_IDX, HEAD_IDX), (9, 15)], # second one on this one IOI prompt only...\n",
    "}[model.cfg.model_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep some bags of words...\n",
    "# OVERLY LONG because it really helps to have the bags of words the same length\n",
    "\n",
    "bags_of_words = []\n",
    "\n",
    "OUTER_LEN = 50\n",
    "INNER_LEN = 100\n",
    "\n",
    "idx = -1\n",
    "while len(bags_of_words) < OUTER_LEN:\n",
    "    idx += 1\n",
    "    cur_tokens = model.tokenizer.encode(dataset[idx])\n",
    "    cur_bag = []\n",
    "    \n",
    "    for i in range(len(cur_tokens)):\n",
    "        if len(cur_bag) == INNER_LEN:\n",
    "            break\n",
    "        if cur_tokens[i] not in cur_bag:\n",
    "            cur_bag.append(cur_tokens[i])\n",
    "\n",
    "    if len(cur_bag) == INNER_LEN:\n",
    "        bags_of_words.append(cur_bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_effective_embedding(model: HookedTransformer) -> Float[Tensor, \"d_vocab d_model\"]:\n",
    "\n",
    "    W_E = model.W_E.clone()\n",
    "    W_U = model.W_U.clone()\n",
    "    # t.testing.assert_close(W_E[:10, :10], W_U[:10, :10].T)  NOT TRUE, because of the center unembed part!\n",
    "\n",
    "    resid_pre = W_E.unsqueeze(0)\n",
    "    pre_attention = model.blocks[0].ln1(resid_pre)\n",
    "    attn_out = einops.einsum(\n",
    "        pre_attention, \n",
    "        model.W_V[0],\n",
    "        model.W_O[0],\n",
    "        \"b s d_model, num_heads d_model d_head, num_heads d_head d_model_out -> b s d_model_out\",\n",
    "    )\n",
    "    resid_mid = attn_out + resid_pre\n",
    "    normalized_resid_mid = model.blocks[0].ln2(resid_mid)\n",
    "    mlp_out = model.blocks[0].mlp(normalized_resid_mid)\n",
    "    \n",
    "    W_EE = mlp_out.squeeze()\n",
    "    W_EE_full = resid_mid.squeeze() + mlp_out.squeeze()\n",
    "\n",
    "    return {\n",
    "        \"W_U (or W_E, no MLPs)\": W_U.T,\n",
    "        # \"W_E (raw, no MLPs)\": W_E,\n",
    "        \"W_E (including MLPs)\": W_EE_full,\n",
    "        \"W_E (only MLPs)\": W_EE\n",
    "    }\n",
    "\n",
    "embeddings_dict = get_effective_embedding(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting just diag patterns for a single head\n",
    "\n",
    "from transformer_lens import FactoredMatrix\n",
    "\n",
    "LAYER = 3\n",
    "HEAD = 0\n",
    "\n",
    "all_results = []\n",
    "embeddings_dict_keys = sorted(embeddings_dict.keys())\n",
    "labels = []\n",
    "\n",
    "for q_side_matrix, k_side_matrix in tqdm(list(itertools.product(embeddings_dict_keys, embeddings_dict_keys))):\n",
    "    labels.append(f\"Q = {q_side_matrix}<br>K = {k_side_matrix}\")\n",
    "\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            LAYER,\n",
    "            HEAD,\n",
    "            show_plot=False,\n",
    "            num_samples=None,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx: idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[q_side_matrix],\n",
    "            W_E_key_side=embeddings_dict[k_side_matrix],\n",
    "        )\n",
    "        results.append(softmaxed_attn)\n",
    "    \n",
    "    all_results.append(sum(results) / len(results))\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "all_results = t.stack(all_results) # .reshape((3, 3, INNER_LEN, INNER_LEN))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imshow(\n",
    "    all_results,\n",
    "    facet_col=0,\n",
    "    facet_col_wrap=len(embeddings_dict),\n",
    "    facet_labels=labels,\n",
    "    title=f\"Sample of diagonal patterns for different matrices: head 3.0 (duplicate token head)\",\n",
    "    labels={\"x\": \"Key\", \"y\": \"Query\"},\n",
    "    height=900, width=900\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = t.zeros(12, 12).float().to(device)\n",
    "\n",
    "for layer, head in tqdm(list(itertools.product(range(12), range(12)))):\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            layer,\n",
    "            head,\n",
    "            show_plot=False,\n",
    "            num_samples=None,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx:idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[\"W_U (or W_E, no MLPs)\"],\n",
    "            W_E_key_side=embeddings_dict[\"W_E (including MLPs)\"],  # \"W_E (only MLPs)\"\n",
    "        )\n",
    "        results.append(softmaxed_attn.diag().mean())\n",
    "\n",
    "    results = sum(results) / len(results)\n",
    "\n",
    "    scores[layer, head] = results\n",
    "\n",
    "imshow(scores, width=750, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attn scores for bag of words (including MLPs in embedding)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = t.zeros(12, 12).float().to(device)\n",
    "\n",
    "for layer, head in tqdm(list(itertools.product(range(12), range(12)))):\n",
    "    results = []\n",
    "    for idx in range(OUTER_LEN):\n",
    "        softmaxed_attn = get_EE_QK_circuit(\n",
    "            layer,\n",
    "            head,\n",
    "            show_plot=False,\n",
    "            num_samples=None,\n",
    "            random_seeds=None,\n",
    "            bags_of_words=bags_of_words[idx:idx+1],\n",
    "            mean_version=False,\n",
    "            W_E_query_side=embeddings_dict[\"W_U (or W_E, no MLPs)\"],\n",
    "            W_E_key_side=embeddings_dict[\"W_E (only MLPs)\"],  # \n",
    "        )\n",
    "        results.append(softmaxed_attn.diag().mean())\n",
    "\n",
    "    results = sum(results) / len(results)\n",
    "\n",
    "    scores[layer, head] = results\n",
    "\n",
    "imshow(scores, width=750, labels={\"x\": \"Head\", \"y\": \"Layer\"}, title=\"Prediction-attn scores for bag of words (only MLPs in embedding)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
