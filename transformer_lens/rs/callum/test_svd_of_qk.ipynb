{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "# model.set_use_split_qkv_input(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens import FactoredMatrix\n",
    "\n",
    "W_E = model.W_E\n",
    "W_U = model.W_U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate W_{EE} edit\n",
    "batch_size = 1000\n",
    "nrows = model.cfg.d_vocab\n",
    "W_EE = t.zeros((nrows, model.cfg.d_model)).to(device)\n",
    "\n",
    "for i in tqdm(range(0, nrows + batch_size, batch_size)):\n",
    "    cur_range = t.tensor(range(i, min(i + batch_size, nrows)))\n",
    "    if len(cur_range)>0:\n",
    "        embeds = W_E[cur_range].unsqueeze(0)\n",
    "        pre_attention = model.blocks[0].ln1(embeds)\n",
    "        post_attention = einops.einsum(\n",
    "            pre_attention, \n",
    "            model.W_V[0],\n",
    "            model.W_O[0],\n",
    "            \"b s d_model, num_heads d_model d_head, num_heads d_head d_model_out -> b s d_model_out\",\n",
    "        )\n",
    "        normalized_resid_mid = model.blocks[0].ln2(post_attention + embeds)\n",
    "        resid_post = model.blocks[0].mlp(normalized_resid_mid)\n",
    "        W_EE[cur_range.to(device)] = resid_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_QK = FactoredMatrix(model.W_Q[10, 7], model.W_K[10, 7].T)\n",
    "\n",
    "W_QK_full = W_E @ W_QK @ W_E.T\n",
    "W_QK_full_eff = W_U.T @ W_QK @ W_EE.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(W_QK.S, height=400, width=600, title=\"W_QK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(W_QK_full.S, height=400, width=600, title=\"W_QK full (W_E on both sides)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "line(W_QK_full_eff.S, height=400, width=600, title=\"W_QK effective (W_U on query, W_EE on key)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens(W_QK_full_eff.U[:, 0].abs().argmax())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to_str_tokens(W_QK_full_eff.Vh[:, 0].abs().argmax())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# High cosine sim between names and the principal directions of $W_{QK}$ ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_QK.U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "squared_cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_U_normed = W_U / W_U.norm(dim=0)\n",
    "\n",
    "squared_cos_sim = (W_U.T @ W_QK.U[:, 0]) ** 2\n",
    "max_cos_sim_words = squared_cos_sim.topk(10).indices\n",
    "\n",
    "model.to_str_tokens(max_cos_sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_EE_normed = W_EE / W_EE.norm(dim=-1, keepdim=True)\n",
    "\n",
    "squared_cos_sim = (W_QK.Vh[:, 0] @ W_EE_normed.T) ** 2\n",
    "max_cos_sim_words = squared_cos_sim.topk(10).indices\n",
    "\n",
    "model.to_str_tokens(max_cos_sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_E_normed = W_E / W_E.norm(dim=-1, keepdim=True)\n",
    "\n",
    "squared_cos_sim = (W_QK.Vh[:, 0] @ W_E_normed.T) ** 2\n",
    "max_cos_sim_words = squared_cos_sim.topk(10).indices\n",
    "\n",
    "model.to_str_tokens(max_cos_sim_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_QK.U.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_cos_sims = []\n",
    "\n",
    "for name in NAMES:\n",
    "\n",
    "    unit_unembedding_vector = W_U[:, model.to_single_token(name)]\n",
    "    unit_unembedding_vector /= unit_unembedding_vector.norm()\n",
    "\n",
    "    variance_of_unembedding_explained_by_left_singular_space = einops.einsum(\n",
    "        unit_unembedding_vector,\n",
    "        W_QK.U,\n",
    "        \"d_model, d_model d_vocab -> d_vocab\"\n",
    "    ).pow(2).sum()\n",
    "\n",
    "    name_cos_sims.append(variance_of_unembedding_explained_by_left_singular_space.item())\n",
    "\n",
    "hist(np.array(name_cos_sims) * 12, template=\"simple_white\", width=800)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generic_words = [\n",
    "    \"the\", \"be\", \"to\", \"of\", \"and\", \"a\", \"in\", \"that\", \"have\", \"I\", \"it\", \"for\", \"not\", \"on\", \"with\", \"he\", \"as\", \"you\", \"do\", \"at\", \"this\", \"but\", \"his\", \"by\", \"from\", \"they\", \"we\", \"say\", \"her\", \"she\", \"or\", \"an\", \"will\", \"my\", \"one\", \n",
    "    \"all\", \"would\", \"there\", \"their\", \"what\", \"so\", \"up\", \"out\", \"if\", \"about\", \"who\", \"get\", \"which\", \"go\", \"me\", \"when\", \"make\", \"can\", \"like\", \"time\", \"no\", \"just\", \"him\", \"know\", \"take\", \"people\", \"into\", \"year\", \"your\", \"good\", \"some\", \n",
    "    \"could\", \"them\", \"see\", \"other\", \"than\", \"then\", \"now\", \"look\", \"only\", \"come\", \"its\", \"over\", \"think\", \"also\", \"back\", \"after\", \"use\", \"two\", \"how\", \"our\", \"work\", \"first\", \"well\", \"way\", \"even\", \"new\", \"want\", \"because\", \"any\", \n",
    "    \"these\", \"give\", \"day\", \"most\", \"us\"\n",
    "]\n",
    "\n",
    "generic_word_cos_sims = []\n",
    "\n",
    "for word in generic_words:\n",
    "\n",
    "    unit_unembedding_vector = W_U[:, model.to_single_token(word)]\n",
    "    unit_unembedding_vector /= unit_unembedding_vector.norm()\n",
    "\n",
    "    variance_of_unembedding_explained_by_left_singular_space = einops.einsum(\n",
    "        unit_unembedding_vector,\n",
    "        W_QK.U,\n",
    "        \"d_model, d_model d_vocab -> d_vocab\"\n",
    "    ).pow(2).sum()\n",
    "\n",
    "    generic_word_cos_sims.append(variance_of_unembedding_explained_by_left_singular_space.item())\n",
    "\n",
    "hist(np.array(generic_word_cos_sims) * 12, template=\"simple_white\", width=800)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
