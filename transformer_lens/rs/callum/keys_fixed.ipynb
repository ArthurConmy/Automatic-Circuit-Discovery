{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _logits_to_ave_logit_diff(logits: Float[Tensor, \"batch seq d_vocab\"], ioi_dataset: IOIDataset, per_prompt=False):\n",
    "    '''\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    '''\n",
    "\n",
    "    # Only the final logits are relevant for the answer\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    io_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.io_tokenIDs]\n",
    "    s_logits: Float[Tensor, \"batch\"] = logits[range(logits.size(0)), ioi_dataset.word_idx[\"end\"], ioi_dataset.s_tokenIDs]\n",
    "    # Find logit difference\n",
    "    answer_logit_diff = io_logits - s_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "\n",
    "\n",
    "\n",
    "def _ioi_metric_noising(\n",
    "        logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "        clean_logit_diff: float,\n",
    "        corrupted_logit_diff: float,\n",
    "        ioi_dataset: IOIDataset,\n",
    "    ) -> float:\n",
    "        '''\n",
    "        We calibrate this so that the value is 0 when performance isn't harmed (i.e. same as IOI dataset),\n",
    "        and -1 when performance has been destroyed (i.e. is same as ABC dataset).\n",
    "        '''\n",
    "        patched_logit_diff = _logits_to_ave_logit_diff(logits, ioi_dataset)\n",
    "        return ((patched_logit_diff - clean_logit_diff) / (clean_logit_diff - corrupted_logit_diff)).item()\n",
    "\n",
    "\n",
    "\n",
    "def generate_data_and_caches(N: int, verbose: bool = False, seed: int = 42):\n",
    "\n",
    "    ioi_dataset = IOIDataset(\n",
    "        prompt_type=\"mixed\",\n",
    "        N=N,\n",
    "        tokenizer=model.tokenizer,\n",
    "        prepend_bos=False,\n",
    "        seed=seed,\n",
    "        device=str(device)\n",
    "    )\n",
    "\n",
    "    abc_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->XYZ, BAB->XYZ\")\n",
    "\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "\n",
    "    ioi_logits_original, ioi_cache = model.run_with_cache(ioi_dataset.toks)\n",
    "    abc_logits_original, abc_cache = model.run_with_cache(abc_dataset.toks)\n",
    "\n",
    "    ioi_average_logit_diff = _logits_to_ave_logit_diff(ioi_logits_original, ioi_dataset).item()\n",
    "    abc_average_logit_diff = _logits_to_ave_logit_diff(abc_logits_original, ioi_dataset).item()\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Average logit diff (IOI dataset): {ioi_average_logit_diff:.4f}\")\n",
    "        print(f\"Average logit diff (ABC dataset): {abc_average_logit_diff:.4f}\")\n",
    "\n",
    "    ioi_metric_noising = partial(\n",
    "        _ioi_metric_noising,\n",
    "        clean_logit_diff=ioi_average_logit_diff,\n",
    "        corrupted_logit_diff=abc_average_logit_diff,\n",
    "        ioi_dataset=ioi_dataset,\n",
    "    )\n",
    "\n",
    "    return ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric_noising\n",
    "\n",
    "\n",
    "N = 100\n",
    "ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric_noising = generate_data_and_caches(N, verbose=True)\n",
    "seq_len = ioi_dataset.toks.size(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_TOKENS = model.to_tokens(NAMES, prepend_bos=False).squeeze().tolist()\n",
    "NNMH = [(10, 7), (11, 0)]\n",
    "\n",
    "def attn_scores_as_linear_func_of_keys(\n",
    "    batch_idx: Union[int, List[int], Int[Tensor, \"batch\"]] = None,\n",
    "    head: Tuple[int, int] = NNMH[0],\n",
    "    model: HookedTransformer = model,\n",
    "    ioi_cache: ActivationCache = ioi_cache\n",
    ") -> Float[Tensor, \"d_model\"]:\n",
    "    '''\n",
    "    If you hold keys fixed, then attention scores are a linear function of the keys.\n",
    "\n",
    "    I want to fix the keys of head 10.7, and get a linear function mapping queries -> attention scores.\n",
    "\n",
    "    I can then see if (for example) the unembedding vector for the IO token has a really big image in this linear fn.\n",
    "    '''\n",
    "    layer, head_idx = head\n",
    "    if isinstance(batch_idx, int):\n",
    "        batch_idx = [batch_idx]\n",
    "    if batch_idx is None:\n",
    "        batch_idx = range(len(ioi_cache[\"q\", 0]))\n",
    "\n",
    "    keys = ioi_cache[\"k\", layer][:, :, head_idx] # shape (all_batch, seq_K, d_head)\n",
    "    keys_at_IO = keys[batch_idx, ioi_dataset.word_idx[\"IO\"][batch_idx]] # shape (batch, d_head)\n",
    "    \n",
    "    W_Q = model.W_Q[layer, head_idx].clone() # shape (d_model, d_head)\n",
    "\n",
    "    linear_map = einops.einsum(W_Q, keys_at_IO, \"d_model d_head, batch d_head -> batch d_model\")\n",
    "    if isinstance(batch_idx, int):\n",
    "        linear_map = linear_map[0]\n",
    "    return linear_map\n",
    "\n",
    "\n",
    "attn_scores_IO = t.empty((0,)).to(device)\n",
    "attn_scores_S = t.empty((0,)).to(device)\n",
    "attn_scores_random = t.empty((0,)).to(device)\n",
    "attn_scores_random_name = t.empty((0,)).to(device)\n",
    "attn_scores_99_out = t.empty((0,)).to(device)\n",
    "\n",
    "probs_list = [t.empty((0,)).to(device), t.empty((0,)).to(device), t.empty((0,)).to(device), t.empty((0,)).to(device)]\n",
    "\n",
    "for seed in tqdm(range(10)):\n",
    "\n",
    "    ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric_noising = generate_data_and_caches(N, seed=seed)\n",
    "\n",
    "    linear_map = attn_scores_as_linear_func_of_keys(ioi_cache=ioi_cache)\n",
    "    assert linear_map.shape == (N, model.cfg.d_model)\n",
    "\n",
    "    # Has to be manual, because apparently `apply_ln_to_stack` doesn't allow it to be applied at different sequence positions\n",
    "    # Note - I don't actually have to do this if I'm computing cosine similarity!\n",
    "    io_unembeddings = model.W_U.T[t.tensor(ioi_dataset.io_tokenIDs)]\n",
    "    s_unembeddings = model.W_U.T[t.tensor(ioi_dataset.s_tokenIDs)]\n",
    "    random_unembeddings = model.W_U.T[t.randint(size=(N,), low=0, high=model.cfg.d_vocab)]\n",
    "    random_name_unembeddings = model.W_U.T[np.random.choice(NAME_TOKENS, size=(N,))]\n",
    "    ln_scales = ioi_cache[\"scale\", 10, \"ln2\"][range(N), ioi_dataset.word_idx[\"end\"]]\n",
    "    out_99 = einops.einsum(ioi_cache[\"z\", 9][range(N), ioi_dataset.word_idx[\"end\"], 9], model.W_O[9, 9], \"batch d_head, d_head d_model -> batch d_model\")\n",
    "\n",
    "    io_unembeddings_normalized = io_unembeddings / io_unembeddings.norm(dim=-1, keepdim=True)\n",
    "    s_unembeddings_normalized = s_unembeddings / s_unembeddings.norm(dim=-1, keepdim=True)\n",
    "    random_unembeddings_normalized = random_unembeddings / random_unembeddings.norm(dim=-1, keepdim=True)\n",
    "    random_name_unembeddings_normalized = random_name_unembeddings / random_name_unembeddings.norm(dim=-1, keepdim=True)\n",
    "    out_99_normalized = out_99 / out_99.norm(dim=-1, keepdim=True)\n",
    "    # io_unembeddings_normalized = io_unembeddings / ln_scales\n",
    "    # s_unembeddings_normalized = s_unembeddings / ln_scales\n",
    "    # random_unembeddings_normalized = random_unembeddings / ln_scales\n",
    "    # random_name_unembeddings_normalized = random_name_unembeddings / ln_scales\n",
    "    # out_99_normalized = out_99 / ln_scales\n",
    "\n",
    "    new_attn_scores_IO = einops.einsum(linear_map, io_unembeddings_normalized, \"batch d_model, batch d_model -> batch\")\n",
    "    attn_scores_IO = t.concat([attn_scores_IO, new_attn_scores_IO])\n",
    "    \n",
    "    new_attn_scores_S = einops.einsum(linear_map, s_unembeddings_normalized, \"batch d_model, batch d_model -> batch\")\n",
    "    attn_scores_S = t.concat([attn_scores_S, new_attn_scores_S])\n",
    "\n",
    "    new_attn_scores_random = einops.einsum(linear_map, random_unembeddings_normalized, \"batch d_model, batch d_model -> batch\")\n",
    "    attn_scores_random = t.concat([attn_scores_random, new_attn_scores_random])\n",
    "\n",
    "    new_attn_scores_random_name = einops.einsum(linear_map, random_name_unembeddings_normalized, \"batch d_model, batch d_model -> batch\")\n",
    "    attn_scores_random_name = t.concat([attn_scores_random_name, new_attn_scores_random_name])\n",
    "\n",
    "    new_attn_scores_99_out = einops.einsum(linear_map, out_99_normalized, \"batch d_model, batch d_model -> batch\")\n",
    "    attn_scores_99_out = t.concat([attn_scores_99_out, new_attn_scores_99_out])\n",
    "\n",
    "    other_attn_scores_at_this_posn = ioi_cache[\"attn_scores\", 10][range(N), 7, ioi_dataset.word_idx[\"end\"]]\n",
    "\n",
    "    # for i, new_attn_scores in enumerate([new_attn_scores_IO, new_attn_scores_S, new_attn_scores_random, new_attn_scores_random_name, new_attn_scores_99_out]):\n",
    "    #     all_attn_scores = other_attn_scores_at_this_posn.clone()\n",
    "    #     all_attn_scores[range(N), ioi_dataset.word_idx[\"IO\"]] = new_attn_scores\n",
    "    #     all_probs = all_attn_scores.softmax(dim=-1)[range(N), ioi_dataset.word_idx[\"IO\"]]\n",
    "    #     probs_list[i] = t.cat([probs_list[i], all_probs])\n",
    "\n",
    "# probs_IO, probs_S, probs_random, probs_random_name, probs_99_out = probs_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(\n",
    "    [attn_scores_IO, attn_scores_99_out, attn_scores_S, attn_scores_random_name, attn_scores_random],\n",
    "    labels={\"variable\": \"Query-side vector\", \"value\": \"Attention scores\"},\n",
    "    title=\"Attn scores (from END -> IO) in NNMH 10.7 (keys fixed, scores are linear func of queries)\",\n",
    "    names=[\"W_U[IO]\", \"NMH 9.9 output\", \"W_U[S]\", \"W_U[random name]\", \"W_U[random]\"],\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\",\n",
    "    nbins=100\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = t.optim.Adam([t.ones(3, 4, requires_grad=True)])\n",
    "p.param_groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(\n",
    "    [probs_IO, probs_S, probs_random_name, probs_random],\n",
    "    labels={\"variable\": \"Query-side vector\", \"value\": \"Attention prob\"},\n",
    "    title=\"Attn probs (from END -> IO) in NNMH 10.7 (keys fixed, query-side vector patched in)\",\n",
    "    names=[\"W_U[IO]\", \"W_U[S]\", \"W_U[random name]\", \"W_U[random]\"],\n",
    "    width=1000,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
