{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"solu-10l\", # \"NeelNanda/SoLU_10L1280W_C4_Code\"\n",
    "    # device=\"cpu\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "# model.set_use_split_qkv_input(True)\n",
    "# model = model.to(\"cuda\")\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_prompt = \"After John and Mary went to the store, John gave a bottle of milk to\"\n",
    "example_answer = \" Mary\"\n",
    "utils.test_prompt(example_prompt, example_answer, model, prepend_bos=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_format = [\n",
    "    \"When John and Mary went to the shops,{} gave the bag to\",\n",
    "    \"When Tom and James went to the park,{} gave the ball to\",\n",
    "    \"When Dan and Sid went to the shops,{} gave an apple to\",\n",
    "    \"After Martin and Amy went to the park,{} gave a drink to\",\n",
    "]\n",
    "name_pairs = [\n",
    "    (\" Mary\", \" John\"),\n",
    "    (\" Tom\", \" James\"),\n",
    "    (\" Dan\", \" Sid\"),\n",
    "    (\" Martin\", \" Amy\"),\n",
    "]\n",
    "\n",
    "# Define 8 prompts, in 4 groups of 2 (with adjacent prompts having answers swapped)\n",
    "prompts = [\n",
    "    prompt.format(name)\n",
    "    for (prompt, names) in zip(prompt_format, name_pairs) for name in names[::-1]\n",
    "]\n",
    "# Define the answers for each prompt, in the form (correct, incorrect)\n",
    "answers = [names[::i] for names in name_pairs for i in (1, -1)]\n",
    "# Define the answer tokens (same shape as the answers)\n",
    "answer_tokens = t.concat([\n",
    "    model.to_tokens(names, prepend_bos=False).T for names in answers\n",
    "])\n",
    "\n",
    "tokens = model.to_tokens(prompts, prepend_bos=True)\n",
    "# Move the tokens to the GPU\n",
    "tokens = tokens.to(device)\n",
    "# Run the model and cache all activations\n",
    "original_logits, cache = model.run_with_cache(tokens)\n",
    "\n",
    "def logits_to_ave_logit_diff(\n",
    "    logits: Float[t.Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[t.Tensor, \"batch 2\"] = answer_tokens,\n",
    "    per_prompt: bool = False\n",
    "):\n",
    "    '''\n",
    "    Returns logit difference between the correct and incorrect answer.\n",
    "\n",
    "    If per_prompt=True, return the array of differences rather than the average.\n",
    "    '''\n",
    "    # Only the final logits are relevant for the answer\n",
    "    final_logits: Float[t.Tensor, \"batch d_vocab\"] = logits[:, -1, :]\n",
    "    # Get the logits corresponding to the indirect object / subject tokens respectively\n",
    "    answer_logits: Float[t.Tensor, \"batch 2\"] = final_logits.gather(dim=-1, index=answer_tokens)\n",
    "    # Find logit difference\n",
    "    correct_logits, incorrect_logits = answer_logits.unbind(dim=-1)\n",
    "    answer_logit_diff = correct_logits - incorrect_logits\n",
    "    return answer_logit_diff if per_prompt else answer_logit_diff.mean()\n",
    "\n",
    "original_per_prompt_diff = logits_to_ave_logit_diff(original_logits, answer_tokens, per_prompt=True)\n",
    "print(\"Per prompt logit difference:\", original_per_prompt_diff)\n",
    "original_average_logit_diff = logits_to_ave_logit_diff(original_logits, answer_tokens)\n",
    "print(\"Average logit difference:\", original_average_logit_diff)\n",
    "\n",
    "answer_residual_directions: Float[t.Tensor, \"batch 2 d_model\"] = model.tokens_to_residual_directions(answer_tokens)\n",
    "print(\"Answer residual directions shape:\", answer_residual_directions.shape)\n",
    "\n",
    "correct_residual_directions, incorrect_residual_directions = answer_residual_directions.unbind(dim=1)\n",
    "logit_diff_directions: Float[t.Tensor, \"batch d_model\"] = correct_residual_directions - incorrect_residual_directions\n",
    "print(f\"Logit difference directions shape:\", logit_diff_directions.shape)\n",
    "\n",
    "# cache syntax - resid_post is the residual stream at the end of the layer, -1 gets the final layer. The general syntax is [activation_name, layer_index, sub_layer_type].\n",
    "final_residual_stream: Float[Tensor, \"batch seq d_model\"] = cache[\"resid_post\", -1]\n",
    "print(f\"Final residual stream shape: {final_residual_stream.shape}\")\n",
    "final_token_residual_stream: Float[Tensor, \"batch d_model\"] = final_residual_stream[:, -1, :]\n",
    "\n",
    "# Apply LayerNorm scaling (to just the final sequence position)\n",
    "# pos_slice is the subset of the positions we take - here the final token of each prompt\n",
    "scaled_final_token_residual_stream = cache.apply_ln_to_stack(final_token_residual_stream, layer=-1, pos_slice=-1)\n",
    "\n",
    "average_logit_diff = einops.einsum(\n",
    "    scaled_final_token_residual_stream, logit_diff_directions,\n",
    "    \"batch d_model, batch d_model ->\"\n",
    ") / len(prompts)\n",
    "\n",
    "print(f\"Calculated average logit diff: {average_logit_diff:.10f}\")\n",
    "print(f\"Original logit difference:     {original_average_logit_diff:.10f}\")\n",
    "\n",
    "t.testing.assert_close(average_logit_diff, original_average_logit_diff)\n",
    "\n",
    "def residual_stack_to_logit_diff(\n",
    "    residual_stack: Float[Tensor, \"... batch d_model\"],\n",
    "    cache: ActivationCache,\n",
    "    logit_diff_directions: Float[Tensor, \"batch d_model\"] = logit_diff_directions,\n",
    ") -> Float[Tensor, \"...\"]:\n",
    "    '''\n",
    "    Gets the avg logit difference between the correct and incorrect answer for a given\n",
    "    stack of components in the residual stream.\n",
    "    '''\n",
    "    # SOLUTION\n",
    "    batch_size = residual_stack.size(-2)\n",
    "    scaled_residual_stack = cache.apply_ln_to_stack(residual_stack, layer=-1, pos_slice=-1)\n",
    "    return einops.einsum(\n",
    "        scaled_residual_stack, logit_diff_directions,\n",
    "        \"... batch d_model, batch d_model -> ...\"\n",
    "    ) / batch_size\n",
    "\n",
    "\n",
    "# Test function by checking that it gives the same result as the original logit difference\n",
    "t.testing.assert_close(\n",
    "    residual_stack_to_logit_diff(final_token_residual_stream, cache),\n",
    "    original_average_logit_diff\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 50\n",
    "ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric_noising= generate_data_and_caches(N, model, verbose=True, seed=43)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1️⃣ Logit Attribution & Activation Patching & Attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accumulated_residual, labels = cache.accumulated_resid(layer=-1, incl_mid=True, pos_slice=-1, return_labels=True)\n",
    "# accumulated_residual has shape (component, batch, d_model)\n",
    "\n",
    "logit_lens_logit_diffs: Float[Tensor, \"component\"] = residual_stack_to_logit_diff(accumulated_residual, cache)\n",
    "\n",
    "line(\n",
    "    logit_lens_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Accumulated Residual Stream\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_layer_residual, labels = cache.decompose_resid(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_layer_logit_diffs = residual_stack_to_logit_diff(per_layer_residual, cache)\n",
    "\n",
    "line(\n",
    "    per_layer_logit_diffs,\n",
    "    hovermode=\"x unified\",\n",
    "    title=\"Logit Difference From Each Layer\",\n",
    "    labels={\"x\": \"Layer\", \"y\": \"Logit Diff\"},\n",
    "    xaxis_tickvals=labels,\n",
    "    width=800\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "per_head_residual, labels = cache.stack_head_results(layer=-1, pos_slice=-1, return_labels=True)\n",
    "per_head_residual = einops.rearrange(\n",
    "    per_head_residual,\n",
    "    \"(layer head) ... -> layer head ...\",\n",
    "    layer=model.cfg.n_layers\n",
    ")\n",
    "per_head_logit_diffs = residual_stack_to_logit_diff(per_head_residual, cache)\n",
    "\n",
    "imshow(\n",
    "    per_head_logit_diffs,\n",
    "    labels={\"x\":\"Head\", \"y\":\"Layer\"},\n",
    "    title=\"Logit Difference From Each Head\",\n",
    "    width=600\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_tokens = tokens\n",
    "# Swap each adjacent pair to get corrupted tokens\n",
    "indices = [i+1 if i % 2 == 0 else i-1 for i in range(len(tokens))]\n",
    "corrupted_tokens = clean_tokens[indices]\n",
    "\n",
    "print(\n",
    "    \"Clean string 0:    \", model.to_string(clean_tokens[0]), \"\\n\"\n",
    "    \"Corrupted string 0:\", model.to_string(corrupted_tokens[0])\n",
    ")\n",
    "\n",
    "clean_logits, clean_cache = model.run_with_cache(clean_tokens)\n",
    "corrupted_logits, corrupted_cache = model.run_with_cache(corrupted_tokens)\n",
    "\n",
    "clean_logit_diff = logits_to_ave_logit_diff(clean_logits, answer_tokens)\n",
    "print(f\"Clean logit diff: {clean_logit_diff:.4f}\")\n",
    "\n",
    "corrupted_logit_diff = logits_to_ave_logit_diff(corrupted_logits, answer_tokens)\n",
    "print(f\"Corrupted logit diff: {corrupted_logit_diff:.4f}\")\n",
    "\n",
    "def ioi_metric(\n",
    "    logits: Float[Tensor, \"batch seq d_vocab\"],\n",
    "    answer_tokens: Float[Tensor, \"batch 2\"] = answer_tokens,\n",
    "    corrupted_logit_diff: float = corrupted_logit_diff,\n",
    "    clean_logit_diff: float = clean_logit_diff,\n",
    ") -> Float[Tensor, \"\"]:\n",
    "    '''\n",
    "    Linear function of logit diff, calibrated so that it equals 0 when performance is\n",
    "    same as on corrupted input, and 1 when performance is same as on clean input.\n",
    "    '''\n",
    "    # SOLUTION\n",
    "    patched_logit_diff = logits_to_ave_logit_diff(logits, answer_tokens)\n",
    "    return ((patched_logit_diff - corrupted_logit_diff) / (clean_logit_diff  - corrupted_logit_diff)).item()\n",
    "\n",
    "t.testing.assert_close(ioi_metric(clean_logits), 1.0)\n",
    "t.testing.assert_close(ioi_metric(corrupted_logits), 0.0)\n",
    "t.testing.assert_close(ioi_metric((clean_logits + corrupted_logits) / 2), 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = act_patch(\n",
    "    model = model,\n",
    "    orig_input = corrupted_tokens,\n",
    "    patching_nodes = IterNode([\"resid_pre\"], seq_pos=\"each\"),\n",
    "    patching_metric = ioi_metric,\n",
    "    new_input = clean_tokens,\n",
    "    new_cache = clean_cache,\n",
    "    verbose = True\n",
    ")\n",
    "labels = [f\"{tok} {i}\" for i, tok in enumerate(model.to_str_tokens(clean_tokens[0]))]\n",
    "\n",
    "imshow(\n",
    "    results[\"resid_pre\"].T,\n",
    "    labels={\"x\": \"Position\", \"y\": \"Layer\"},\n",
    "    x=labels,\n",
    "    title=\"resid_pre Activation Patching\",\n",
    "    width=500,\n",
    "    border=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def topk_of_Nd_tensor(tensor: Float[Tensor, \"rows cols\"], k: int):\n",
    "    '''\n",
    "    Helper function: does same as tensor.topk(k).indices, but works over 2D tensors.\n",
    "    Returns a list of indices, i.e. shape [k, tensor.ndim].\n",
    "\n",
    "    Example: if tensor is 2D array of values for each head in each layer, this will\n",
    "    return a list of heads.\n",
    "    '''\n",
    "    i = t.topk(tensor.flatten(), k).indices\n",
    "    return np.array(np.unravel_index(utils.to_numpy(i), tensor.shape)).T.tolist()\n",
    "\n",
    "k = 3\n",
    "\n",
    "for head_type in [\"Positive\", \"Negative\"]:\n",
    "\n",
    "    # Get the heads with largest (or smallest) contribution to the logit difference\n",
    "    top_heads = topk_of_Nd_tensor(per_head_logit_diffs * (1 if head_type==\"Positive\" else -1), k)\n",
    "\n",
    "    # Get all their attention patterns\n",
    "    attn_patterns_for_important_heads: Float[Tensor, \"head q k\"] = t.stack([\n",
    "        cache[\"pattern\", layer][:, head][0]\n",
    "        for layer, head in top_heads\n",
    "    ])\n",
    "\n",
    "    # Display results\n",
    "    display(HTML(f\"<h2>Top {k} {head_type} Logit Attribution Heads</h2>\"))\n",
    "    display(cv.attention.attention_patterns(\n",
    "        attention = attn_patterns_for_important_heads,\n",
    "        tokens = model.to_str_tokens(tokens[0]),\n",
    "        attention_head_names = [f\"{layer}.{head}\" for layer, head in top_heads],\n",
    "    ))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3️⃣ Histogram experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NAME_MOVERS = [(7, 3), (7, 5), (7, 12), (8, 0), (8, 2), (8, 3), (8, 5), (8, 17)]\n",
    "NEG_NAME_MOVERS = [(9, 18)]\n",
    "\n",
    "\n",
    "def get_io_vs_s_attn_for_nmh(\n",
    "    patched_cache: ActivationCache,\n",
    "    orig_dataset: IOIDataset,\n",
    "    orig_cache: ActivationCache,\n",
    "    neg_nmh: Tuple[int, int],\n",
    ") -> Float[Tensor, \"batch\"]:\n",
    "    '''\n",
    "    Returns the difference between patterns[END, IO] and patterns[END, S1], where patterns\n",
    "    are the attention patterns for the negative name mover head.\n",
    "\n",
    "    This is returned in the form of a tuple of 2 tensors: one for the patched distribution\n",
    "    (calculated using `patched_cache` which is returned by the path patching algorithm), and\n",
    "    one for the clean IOI distribution (which is just calculated directly from that cache).\n",
    "    '''\n",
    "    layer, head = neg_nmh\n",
    "    attn_pattern_patched = patched_cache[\"pattern\", layer][:, head]\n",
    "    attn_pattern_clean = orig_cache[\"pattern\", layer][:, head]\n",
    "    # display(HTML(f\"<h2>Attn patterns for head {layer}.{head}, clean</h2>\"))\n",
    "    # display(cv.attention.attention_patterns(\n",
    "    #     attention = attn_pattern_clean[0].unsqueeze(0),\n",
    "    #     tokens = model.to_str_tokens(orig_dataset.toks[0]),\n",
    "    # ))\n",
    "    # both are (batch, seq_Q, seq_K), and I want all the \"end -> IO\" attention probs\n",
    "\n",
    "    N = orig_dataset.toks.size(0)\n",
    "    io_seq_pos = orig_dataset.word_idx[\"IO\"]\n",
    "    s1_seq_pos = orig_dataset.word_idx[\"S1\"]\n",
    "    end_seq_pos = orig_dataset.word_idx[\"end\"]\n",
    "\n",
    "    return (\n",
    "        attn_pattern_patched[range(N), end_seq_pos, io_seq_pos] - attn_pattern_patched[range(N), end_seq_pos, s1_seq_pos],\n",
    "        attn_pattern_clean[range(N), end_seq_pos, io_seq_pos] - attn_pattern_clean[range(N), end_seq_pos, s1_seq_pos],\n",
    "    )\n",
    "\n",
    "\n",
    "def get_nnmh_patching_patterns(num_batches = 5, N = 5, neg_nmh = NEG_NAME_MOVERS[0], orig_is_ioi = True):\n",
    "    results_patched = t.empty(size=(0,)).to(device)\n",
    "    results_clean = t.empty(size=(0,)).to(device)\n",
    "\n",
    "    for seed in tqdm(range(num_batches)):\n",
    "\n",
    "        ioi_dataset, abc_dataset, ioi_cache, abc_cache, ioi_metric = generate_data_and_caches(N, model=model, seed=seed, prepend_bos=True)\n",
    "\n",
    "        if orig_is_ioi:\n",
    "            orig_dataset, orig_cache = ioi_dataset, ioi_cache\n",
    "            new_dataset, new_cache = abc_dataset, abc_cache\n",
    "        else:\n",
    "            orig_dataset, orig_cache = abc_dataset, abc_cache\n",
    "            new_dataset, new_cache = ioi_dataset, ioi_cache\n",
    "\n",
    "        new_results_patched, new_results_clean = path_patch(\n",
    "            model,\n",
    "            orig_input=orig_dataset.toks,\n",
    "            new_input=new_dataset.toks,\n",
    "            orig_cache=orig_cache,\n",
    "            new_cache=new_cache,\n",
    "            sender_nodes=[Node(\"z\", layer=layer, head=head) for layer, head in NAME_MOVERS], # Output of all name mover heads\n",
    "            receiver_nodes=Node(\"q\", layer=neg_nmh[0], head=neg_nmh[1]), # To query input of negative name mover head\n",
    "            patching_metric=partial(get_io_vs_s_attn_for_nmh, orig_dataset=orig_dataset, orig_cache=orig_cache, neg_nmh=neg_nmh),\n",
    "            apply_metric_to_cache=True,\n",
    "            direct_includes_mlps=not(model.cfg.use_split_qkv_input),\n",
    "        )\n",
    "        results_patched = t.concat([results_patched, new_results_patched])\n",
    "        results_clean = t.concat([results_clean, new_results_clean])\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "    return results_patched, results_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_patched, results_clean = get_nnmh_patching_patterns(neg_nmh = NEG_NAME_MOVERS[0], num_batches = 100, N = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hist(\n",
    "    [results_patched, results_clean],\n",
    "    labels={\"variable\": \"Version\", \"value\": \"Attn diff (positive ⇒ more attn paid to IO than S1)\"},\n",
    "    title=\"Difference in attn from END➔IO vs. END➔S1 (path-patched vs clean)\",\n",
    "    names=[\"Patched\", \"Clean\"],\n",
    "    width=800,\n",
    "    height=600,\n",
    "    opacity=0.7,\n",
    "    marginal=\"box\",\n",
    "    template=\"simple_white\"\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
