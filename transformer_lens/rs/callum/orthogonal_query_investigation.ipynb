{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "from transformer_lens.rs.callum.keys_fixed import (\n",
    "    attn_scores_as_linear_func_of_keys,\n",
    "    attn_scores_as_linear_func_of_queries,\n",
    "    get_attn_scores_as_linear_func_of_queries_for_histogram,\n",
    "    get_attn_scores_as_linear_func_of_keys_for_histogram,\n",
    "    decompose_attn_scores,\n",
    "    plot_contribution_to_attn_scores,\n",
    "    project,\n",
    "    decompose_attn_scores_full,\n",
    "    create_fucking_massive_plot_1,\n",
    "    create_fucking_massive_plot_2,\n",
    "    get_effective_embedding_2,\n",
    ")\n",
    "\n",
    "# effective_embeddings = get_effective_embedding(model) \n",
    "\n",
    "# W_U = effective_embeddings[\"W_U (or W_E, no MLPs)\"]\n",
    "# W_EE = effective_embeddings[\"W_E (including MLPs)\"]\n",
    "# W_EE_subE = effective_embeddings[\"W_E (only MLPs)\"]\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_split_qkv_normalized_input(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "NEG_NMH = (10, 7)\n",
    "seed = 0\n",
    "\n",
    "ioi_dataset, ioi_cache = generate_data_and_caches(BATCH_SIZE, model=model, seed=seed, only_ioi=True, prepend_bos=True, symmetric=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-a8878c17-61b6\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.39.1/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-a8878c17-61b6\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"Then\", \",\", \" Sarah\", \" and\", \" Arthur\", \" had\", \" a\", \" lot\", \" of\", \" fun\", \" at\", \" the\", \" hospital\", \".\", \" Arthur\", \" gave\", \" a\", \" bone\", \" to\", \" Sarah\", \"<|endoftext|>\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9675536751747131, 0.03244629129767418, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8955413103103638, 0.057365771383047104, 0.04709288850426674, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9532008767127991, 0.009902847930788994, 0.015441919676959515, 0.021454378962516785, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.547920286655426, 0.01625315472483635, 0.01103595644235611, 0.39574694633483887, 0.0290436539798975, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8959264755249023, 0.014111163094639778, 0.025660621002316475, 0.005506726913154125, 0.03541838750243187, 0.02337658777832985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8313417434692383, 0.021181752905249596, 0.02990916185081005, 0.008534150198101997, 0.03786207363009453, 0.013366312719881535, 0.05780477076768875, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8139839172363281, 0.013352294452488422, 0.031910527497529984, 0.001722279703244567, 0.030270712450146675, 0.004387823399156332, 0.053002845495939255, 0.05136961117386818, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9622862339019775, 0.0029872944578528404, 0.006137825082987547, 0.0004197477246634662, 0.003922122996300459, 0.0009028689237311482, 0.013817088678479195, 0.00315750390291214, 0.006369361188262701, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7984487414360046, 0.00986078567802906, 0.018552232533693314, 0.005906171631067991, 0.023182541131973267, 0.006861381232738495, 0.026645014062523842, 0.037098634988069534, 0.0333012193441391, 0.04014328867197037, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8549243807792664, 0.005259246099740267, 0.025354206562042236, 0.00076927529880777, 0.014518211595714092, 0.0008487021550536156, 0.032449208199977875, 0.015032634139060974, 0.0011913921916857362, 0.007668624632060528, 0.04198409616947174, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7866827249526978, 0.007201760075986385, 0.013004140928387642, 0.021039633080363274, 0.023871559649705887, 0.02192431315779686, 0.03224465250968933, 0.024318058043718338, 0.002140742028132081, 0.009150446392595768, 0.013626591302454472, 0.04479537159204483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336069941520691, 0.011740380898118019, 0.01910790242254734, 0.004322831518948078, 0.02473434805870056, 0.009526128880679607, 0.030654113739728928, 0.03437026962637901, 0.0023349991533905268, 0.00837995670735836, 0.014706572517752647, 0.053061213344335556, 0.053454261273145676, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7981045246124268, 0.008875023573637009, 0.020123867318034172, 0.0026317776646465063, 0.02037591114640236, 0.004993693437427282, 0.022569827735424042, 0.005612780340015888, 0.000816409825347364, 0.0023943432606756687, 0.0026014007162302732, 0.02163909375667572, 0.010182433761656284, 0.07907895743846893, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11937661468982697, 0.013665971346199512, 0.01258888840675354, 0.28404727578163147, 0.039001282304525375, 0.4914741814136505, 0.003880272153764963, 0.0025630847085267305, 0.00015126836660783738, 0.0013148847501724958, 0.0009073272231034935, 0.005007469095289707, 0.004463871009647846, 0.0022288404870778322, 0.01932874508202076, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7082263231277466, 0.012075839564204216, 0.025402015075087547, 0.0033906325697898865, 0.0776497945189476, 0.009558046236634254, 0.07971720397472382, 0.006755825597792864, 0.0013361650053411722, 0.005446111783385277, 0.005714281927794218, 0.020121252164244652, 0.007023978512734175, 0.0032435026951134205, 0.022664109244942665, 0.011674879118800163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05173265561461449, 0.005635207053273916, 0.004284646362066269, 0.6935920119285583, 0.01737355999648571, 0.1200629398226738, 0.0030823075212538242, 0.001957083586603403, 0.00012422731379047036, 0.00092738849343732, 0.001125076785683632, 0.0032672386150807142, 0.0027418036479502916, 0.003092896193265915, 0.006812592037022114, 0.07478322833776474, 0.009405121207237244, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6651278734207153, 0.006214347202330828, 0.009749745018780231, 0.003841797588393092, 0.015416835434734821, 0.0033299075439572334, 0.014053051359951496, 0.010713820345699787, 0.003186920192092657, 0.0075620124116539955, 0.06776778399944305, 0.03272295370697975, 0.017168594524264336, 0.0415872186422348, 0.022374659776687622, 0.0024061782751232386, 0.05011792480945587, 0.02665838412940502, 0.0, 0.0, 0.0, 0.0], [0.9202921390533447, 0.00023593244259245694, 0.0025234140921384096, 0.0004967896966263652, 0.00434643030166626, 0.0006345389410853386, 0.0027670073322951794, 0.0006318512023426592, 0.00021805186406709254, 0.0008392421877942979, 0.0009360705153085291, 0.004023062065243721, 0.0013498143525794148, 0.006663188338279724, 0.004644443280994892, 0.0004774023254867643, 0.005400539841502905, 0.001115282066166401, 0.04240481182932854, 0.0, 0.0, 0.0], [0.4411720633506775, 0.01034256350249052, 0.014771167188882828, 0.108586385846138, 0.05356723442673683, 0.07139186561107635, 0.023198937997221947, 0.01135593093931675, 0.0009832769865170121, 0.0064151110127568245, 0.0034326384775340557, 0.01897457428276539, 0.01147582195699215, 0.033171869814395905, 0.031122125685214996, 0.05661015957593918, 0.038306862115859985, 0.008812827989459038, 0.017084119841456413, 0.03922448307275772, 0.0, 0.0], [0.7648003101348877, 0.015192545019090176, 0.024314191192388535, 0.013315361924469471, 0.03420932590961456, 0.019241690635681152, 0.016381626948714256, 0.0029062044341117144, 0.00041567670996300876, 0.0021491420920938253, 0.0009350473992526531, 0.00811181403696537, 0.003698117332533002, 0.002328022150322795, 0.017244024202227592, 0.020677324384450912, 0.016680264845490456, 0.004395289346575737, 0.0020950818434357643, 0.014928754419088364, 0.01598026230931282, 0.0], [0.933887779712677, 0.003894397057592869, 0.003102445276454091, 0.003804840613156557, 0.0014652357203885913, 0.0021726880222558975, 0.0010731227230280638, 0.0007687843753956258, 0.00022487736714538187, 0.0006957747973501682, 0.0002658803132362664, 0.0009133663843385875, 0.0006072969408705831, 0.0007356440764851868, 0.002805259544402361, 0.001729518175125122, 0.0006447556079365313, 0.00043492004624567926, 0.00012139001773903146, 0.0006722683319821954, 0.0010885768570005894, 0.03889107704162598]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7fce14575310>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook_fn_queries(\n",
    "    q_input: Float[Tensor, \"batch seq n_heads d_model\"], \n",
    "    hook: HookPoint,\n",
    "    head: Tuple[int, int] = NEG_NMH,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    model: HookedTransformer = model,\n",
    "    project_in_S_dir: bool = True,\n",
    "    par: bool = False,\n",
    "):\n",
    "    unembed_IO = model.W_U.T[ioi_dataset.io_tokenIDs] # (batch, d_model)\n",
    "    unembed_S = model.W_U.T[ioi_dataset.s_tokenIDs] # (batch, d_model)\n",
    "\n",
    "    proj_dirs = [unembed_IO, unembed_S] if project_in_S_dir else [unembed_IO]\n",
    "    \n",
    "    q_slice = q_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], head[1]]\n",
    "    assert q_slice.shape == unembed_IO.shape\n",
    "    q_input_par, q_input_perp = project(q_slice, proj_dirs)\n",
    "\n",
    "    q_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], head[1]] = (q_input_par if par else q_input_perp)\n",
    "\n",
    "    return q_input\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "model.add_hook(utils.get_act_name(\"q_input\", NEG_NMH[0]), hook_fn_queries)\n",
    "\n",
    "logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"pattern\"))\n",
    "\n",
    "cv.attention.attention_patterns(\n",
    "    attention = cache[\"pattern\", NEG_NMH[0]][0, [NEG_NMH[1]]],\n",
    "    tokens = model.to_str_tokens(ioi_dataset.toks[0]),\n",
    "    # attention_head_names = [\"10.7\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff [qk] = 1.858\n",
      "Diff [q ] = 2.098\n",
      "Diff [ k] = 3.099\n",
      "Diff [  ] = 3.254\n"
     ]
    }
   ],
   "source": [
    "def hook_fn_keys(\n",
    "    k_input: Float[Tensor, \"batch seq n_heads d_model\"], \n",
    "    hook: HookPoint,\n",
    "    head: Tuple[int, int] = NEG_NMH,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    ioi_cache: ActivationCache = ioi_cache,\n",
    "    model: HookedTransformer = model,\n",
    "    project_in_S_dir: bool = True,\n",
    "    par: bool = True,\n",
    "):\n",
    "    N = len(ioi_dataset)\n",
    "    mlp0_dir_IO = ioi_cache[\"mlp_out\", 0][range(N), ioi_dataset.word_idx[\"IO\"]] # (batch, d_model)\n",
    "    mlp0_dir_S = ioi_cache[\"mlp_out\", 0][range(N), ioi_dataset.word_idx[\"S1\"]] # (batch, d_model)\n",
    "\n",
    "    k_input_IO = k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"], head[1]]\n",
    "    k_input_S = k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"], head[1]]\n",
    "\n",
    "    assert k_input_IO.shape == mlp0_dir_IO.shape\n",
    "    k_input_IO_par, k_input_IO_perp = project(k_input_IO, mlp0_dir_IO)\n",
    "    k_input_S_par, k_input_S_perp = project(k_input_S, mlp0_dir_S)\n",
    "\n",
    "    k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"], head[1]] = (k_input_IO_par if par else k_input_IO_perp)\n",
    "    k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"], head[1]] = (k_input_S_par if par else k_input_S_perp)\n",
    "\n",
    "    return k_input\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "q_hook = (utils.get_act_name(\"q_normalized_input\", NEG_NMH[0]), hook_fn_queries)\n",
    "k_hook = (utils.get_act_name(\"k_normalized_input\", NEG_NMH[0]), hook_fn_keys)\n",
    "\n",
    "def test_model(model: HookedTransformer, show_too: bool = False):\n",
    "\n",
    "    for use_q, use_k in itertools.product([True, False], [True, False]):\n",
    "        model.reset_hooks()\n",
    "        if use_q: model.add_hook(*q_hook)\n",
    "        if use_k: model.add_hook(*k_hook)\n",
    "        desc = f\"{'q' if use_q else ' '}{'k' if use_k else ' '}\"\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "        logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"attn_scores\"))\n",
    "        attn_scores = cache[\"attn_scores\", NEG_NMH[0]][:, NEG_NMH[1]]\n",
    "\n",
    "        attn_scores_to_IO = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]]\n",
    "        attn_scores_to_S = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"S1\"]]\n",
    "\n",
    "        print(f\"Diff [{desc}] = {attn_scores_to_IO.mean() - attn_scores_to_S.mean():.3f}\")\n",
    "\n",
    "        if show_too:\n",
    "            labels = [f\"{x}_{i}\" for (i, x) in enumerate(model.to_str_tokens(ioi_dataset.toks[0]))]\n",
    "            imshow(\n",
    "                cache[\"attn_scores\", NEG_NMH[0]][0, NEG_NMH[1]],\n",
    "                x = labels,\n",
    "                y = labels,\n",
    "                labels = {\"x\": \"Key\", \"y\": \"Query\"},\n",
    "                height = 800,\n",
    "            )\n",
    "\n",
    "# cv.attention.attention_patterns(\n",
    "#     attention = cache[\"pattern\", NEG_NMH[0]][0, [NEG_NMH[1]]],\n",
    "#     tokens = model.to_str_tokens(ioi_dataset.toks[0]),\n",
    "#     attention_head_names = [\"10.7\"]\n",
    "# )\n",
    "# labels = [f\"{x}_{i}\" for (i, x) in enumerate(model.to_str_tokens(ioi_dataset.toks[0]))]\n",
    "\n",
    "\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn_patch_wpos_MLP0(\n",
    "    resid_pre: Float[Tensor, \"batch seq d_model\"],\n",
    "    hook: HookPoint,\n",
    "    add: bool,\n",
    "    model: HookedTransformer = model,\n",
    "    permute: bool = False,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "):\n",
    "    seq_len = resid_pre.shape[1]\n",
    "    assert model.W_pos.shape[-1] == model.cfg.d_model\n",
    "    W_pos = model.W_pos[:seq_len]\n",
    "\n",
    "    if permute:\n",
    "        io_posses = W_pos[ioi_dataset.word_idx[\"IO\"]]\n",
    "        s_posses = W_pos[ioi_dataset.word_idx[\"S1\"]]\n",
    "\n",
    "        sign = 1.0 if add else -1.0\n",
    "\n",
    "        shape1 = resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"]].shape\n",
    "        shape2 = s_posses.shape\n",
    "        assert shape1==shape2\n",
    "\n",
    "        resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"]] += sign*(s_posses - io_posses)\n",
    "        resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"]] += sign*(io_posses - s_posses)\n",
    "\n",
    "        return resid_pre\n",
    "\n",
    "    else:\n",
    "        if add:\n",
    "            return resid_pre + W_pos\n",
    "        else:\n",
    "            return resid_pre - W_pos\n",
    "\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "model.add_hook(utils.get_act_name(\"resid_pre\", 0), partial(hook_fn_patch_wpos_MLP0, add=True, permute=True), is_permanent=True)\n",
    "\n",
    "logits, mlp_positional_signals_flipped_cache = model.run_with_cache(ioi_dataset.toks)\n",
    "\n",
    "# model.add_hook(utils.get_act_name(\"resid_pre\", 1), partial(hook_fn_patch_wpos_MLP0, add=False, permute=True), is_permanent=True)\n",
    "\n",
    "# test_model(model, show_too=False)\n",
    "# model.reset_hooks(including_permanent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff [qk] = 1.570\n",
    "# Diff [q ] = 1.711\n",
    "# Diff [ k] = 2.791\n",
    "# Diff [  ] = 2.911"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path patching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Path patch from MLP0 -> keyside 10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->BAB, BAB->ABB\")\n",
    "# flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->BAA, BAB->ABA\")\n",
    "_, flipped_cache = model.run_with_cache(flipped_dataset.toks, names_filter=lambda name: name.endswith(\"mlp_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff [qP] = -0.486\n",
      "Diff [q ] = 2.098\n",
      "Diff [ P] = -0.923\n",
      "Diff [  ] = 3.254\n"
     ]
    }
   ],
   "source": [
    "def patching_metric(cache: ActivationCache) -> float:\n",
    "    attn_scores = cache[\"attn_scores\", 10][:, 7]\n",
    "    attn_scores_to_IO = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]]\n",
    "    attn_scores_to_S = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"S1\"]]\n",
    "    return (attn_scores_to_IO.mean() - attn_scores_to_S.mean()).item()\n",
    "\n",
    "\n",
    "def test_model_PP(model: HookedTransformer):\n",
    "\n",
    "    for use_q, use_PP in itertools.product([True, False], [True, False]):\n",
    "        model.reset_hooks(including_permanent=True)\n",
    "        if use_q:\n",
    "            model.add_hook(*q_hook, is_permanent=True)\n",
    "        desc = f\"{'q' if use_q else ' '}{'P' if use_PP else ' '}\"\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "        if use_PP:\n",
    "            diff = path_patch(\n",
    "                model = model,\n",
    "                patching_metric = patching_metric,\n",
    "                apply_metric_to_cache = True,\n",
    "                orig_input = ioi_dataset.toks,\n",
    "                # new_input = flipped_dataset.toks,\n",
    "                orig_cache = ioi_cache,\n",
    "                new_cache = mlp_positional_signals_flipped_cache,\n",
    "                direct_includes_mlps = True,\n",
    "                sender_nodes = Node(\"mlp_out\", layer=0),\n",
    "                receiver_nodes = [Node(\"v\", layer=9, head=9), Node(\"v\", layer=9, head=6)],\n",
    "                # receiver_nodes = Node(\"k\", layer=10, head=7),\n",
    "            )\n",
    "        else:\n",
    "            # TODO - sanity check, do this with path_patch instead\n",
    "            logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"attn_scores\"))\n",
    "            diff = patching_metric(cache)\n",
    "\n",
    "        print(f\"Diff [{desc}] = {diff:.3f}\")\n",
    "\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "test_model_PP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->CBB, BAB->BCB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset.sentences[0], flipped_dataset.sentences[0]\n",
    "\n",
    "_, flipped_cache = model.run_with_cache(flipped_dataset.toks, names_filter=lambda name: name.endswith(\"k\") and \".10.\" in name)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Act patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "act_patch(\n",
    "    model = model,\n",
    "    orig_input = ioi_dataset.toks,\n",
    "    patching_nodes = Node(\"k\", layer=10, head=7),\n",
    "    new_cache = flipped_cache,\n",
    "    apply_metric_to_cache = True,\n",
    "    patching_metric = patching_metric,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
