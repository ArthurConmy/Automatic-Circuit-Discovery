{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "from transformer_lens.rs.callum.keys_fixed import (\n",
    "    attn_scores_as_linear_func_of_keys,\n",
    "    attn_scores_as_linear_func_of_queries,\n",
    "    get_attn_scores_as_linear_func_of_queries_for_histogram,\n",
    "    get_attn_scores_as_linear_func_of_keys_for_histogram,\n",
    "    decompose_attn_scores,\n",
    "    plot_contribution_to_attn_scores,\n",
    "    project,\n",
    "    decompose_attn_scores_full,\n",
    "    create_fucking_massive_plot_1,\n",
    "    create_fucking_massive_plot_2,\n",
    "    get_effective_embedding_2,\n",
    ")\n",
    "\n",
    "# effective_embeddings = get_effective_embedding(model) \n",
    "\n",
    "# W_U = effective_embeddings[\"W_U (or W_E, no MLPs)\"]\n",
    "# W_EE = effective_embeddings[\"W_E (including MLPs)\"]\n",
    "# W_EE_subE = effective_embeddings[\"W_E (only MLPs)\"]\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "model.set_use_split_qkv_input(True)\n",
    "model.set_use_attn_result(True)\n",
    "model.set_use_split_qkv_normalized_input(True)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 30\n",
    "NEG_NMH = (10, 7)\n",
    "seed = 0\n",
    "\n",
    "ioi_dataset, ioi_cache = generate_data_and_caches(BATCH_SIZE, model=model, seed=seed, only_ioi=True, prepend_bos=True, symmetric=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hook functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div id=\"circuits-vis-5ab61508-de29\" style=\"margin: 15px 0;\"/>\n",
       "    <script crossorigin type=\"module\">\n",
       "    import { render, AttentionPatterns } from \"https://unpkg.com/circuitsvis@1.40.0/dist/cdn/esm.js\";\n",
       "    render(\n",
       "      \"circuits-vis-5ab61508-de29\",\n",
       "      AttentionPatterns,\n",
       "      {\"tokens\": [\"<|endoftext|>\", \"Then\", \",\", \" Sarah\", \" and\", \" Arthur\", \" had\", \" a\", \" lot\", \" of\", \" fun\", \" at\", \" the\", \" hospital\", \".\", \" Arthur\", \" gave\", \" a\", \" bone\", \" to\", \" Sarah\", \"<|endoftext|>\"], \"attention\": [[[1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9675537943840027, 0.032446276396512985, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8955413103103638, 0.05736575648188591, 0.04709291085600853, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.9532008767127991, 0.009902866557240486, 0.015441964380443096, 0.021454375237226486, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.5479208827018738, 0.01625317893922329, 0.011035993695259094, 0.3957462012767792, 0.029043713584542274, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8959264755249023, 0.014111182652413845, 0.02566065825521946, 0.005506721790879965, 0.035418376326560974, 0.02337663248181343, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8313418030738831, 0.021181754767894745, 0.02990921400487423, 0.008534125983715057, 0.03786204010248184, 0.013366295956075191, 0.05780477821826935, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8139835596084595, 0.013352339155972004, 0.03191066160798073, 0.001722278306260705, 0.030270734801888466, 0.004387830384075642, 0.0530029758810997, 0.05136953666806221, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.962286114692688, 0.002987291431054473, 0.006137832999229431, 0.0004197464731987566, 0.003922118339687586, 0.0009028696222230792, 0.013817092403769493, 0.0031575034372508526, 0.006369351409375668, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7984485030174255, 0.009860801510512829, 0.0185522623360157, 0.0059061720967292786, 0.023182544857263565, 0.006861391942948103, 0.026645051315426826, 0.037098634988069534, 0.033301349729299545, 0.04014330357313156, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.8549243807792664, 0.005259246099740267, 0.025354251265525818, 0.0007692763465456665, 0.014518226496875286, 0.0008487053564749658, 0.032449256628751755, 0.015032634139060974, 0.0011913945199921727, 0.007668609730899334, 0.04198404401540756, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7866827249526978, 0.007201756816357374, 0.013004166074097157, 0.021039552986621857, 0.023871533572673798, 0.021924292668700218, 0.032244715839624405, 0.024318069219589233, 0.002140744123607874, 0.009150441735982895, 0.013626597821712494, 0.04479537159204483, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7336073517799377, 0.011740358546376228, 0.019107921048998833, 0.004322829656302929, 0.02473430335521698, 0.00952613353729248, 0.030654113739728928, 0.03437016159296036, 0.0023350047413259745, 0.00837994460016489, 0.014706568792462349, 0.05306108668446541, 0.053454186767339706, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7981044054031372, 0.008875014260411263, 0.020123876631259918, 0.0026317795272916555, 0.020375872030854225, 0.004993699956685305, 0.022569812834262848, 0.005612774286419153, 0.0008164097089320421, 0.0023943413980305195, 0.0026014000177383423, 0.021639080718159676, 0.010182445868849754, 0.0790790468454361, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.11937671154737473, 0.013665950857102871, 0.012588897719979286, 0.2840464413166046, 0.039001256227493286, 0.49147504568099976, 0.003880271455273032, 0.002563083078712225, 0.00015126833750400692, 0.001314885332249105, 0.0009073279798030853, 0.005007477942854166, 0.004463874269276857, 0.002228843281045556, 0.019328737631440163, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.7082263231277466, 0.01207582838833332, 0.025402065366506577, 0.0033906325697898865, 0.07764977216720581, 0.00955805554986, 0.07971726357936859, 0.00675582280382514, 0.001336161745712161, 0.0054461038671433926, 0.005714262370020151, 0.020121274515986443, 0.007023985963314772, 0.003243519924581051, 0.02266407199203968, 0.011674856767058372, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.05173289030790329, 0.005635227542370558, 0.004284666385501623, 0.6935912370681763, 0.017373619601130486, 0.12006328254938126, 0.003082323120906949, 0.001957092434167862, 0.0001242277503479272, 0.0009273923351429403, 0.0011250840034335852, 0.0032672625966370106, 0.002741813426837325, 0.003092916216701269, 0.006812606938183308, 0.07478317618370056, 0.009405172429978848, 0.0, 0.0, 0.0, 0.0, 0.0], [0.6651276350021362, 0.00621434161439538, 0.009749741293489933, 0.003841797588393092, 0.01541685126721859, 0.003329917788505554, 0.014053048565983772, 0.010713801719248295, 0.0031869267113506794, 0.007562009152024984, 0.06776783615350723, 0.03272294998168945, 0.017168624326586723, 0.04158742353320122, 0.022374633699655533, 0.0024061747826635838, 0.05011792480945587, 0.02665834315121174, 0.0, 0.0, 0.0, 0.0], [0.9202921986579895, 0.0002359324716962874, 0.002523411763831973, 0.0004967895220033824, 0.004346420522779226, 0.0006345372530631721, 0.002767002210021019, 0.0006318500963971019, 0.00021805147116538137, 0.0008392410818487406, 0.000936070631723851, 0.004023055080324411, 0.0013498087646439672, 0.006663183216005564, 0.004644435364753008, 0.0004774007829837501, 0.005400517024099827, 0.0011152811348438263, 0.042404767125844955, 0.0, 0.0, 0.0], [0.4411727488040924, 0.010342467576265335, 0.01477113738656044, 0.10858533531427383, 0.05356699973344803, 0.07139220833778381, 0.02319885976612568, 0.011355890892446041, 0.0009832794312387705, 0.006415109150111675, 0.003432678058743477, 0.018974613398313522, 0.011475796811282635, 0.03317195177078247, 0.03112211637198925, 0.05661033093929291, 0.038307011127471924, 0.00881279818713665, 0.017084401100873947, 0.03922423720359802, 0.0, 0.0], [0.7648007273674011, 0.015192548744380474, 0.024314187467098236, 0.013315335847437382, 0.03420926257967949, 0.019241642206907272, 0.016381634399294853, 0.002906203269958496, 0.00041567711741663516, 0.0021491432562470436, 0.000935048796236515, 0.008111829869449139, 0.003698119428008795, 0.0023280256427824497, 0.017243994399905205, 0.020677248015999794, 0.016680283471941948, 0.004395293537527323, 0.0020950871985405684, 0.014928655698895454, 0.015980178490281105, 0.0], [0.9338877201080322, 0.0038944005500525236, 0.0031024536583572626, 0.0038048436399549246, 0.0014652368845418096, 0.002172691747546196, 0.001073122606612742, 0.0007687853649258614, 0.00022487776004709303, 0.0006957747391425073, 0.0002658807788975537, 0.0009133674902841449, 0.0006072968244552612, 0.0007356447167694569, 0.0028052562847733498, 0.001729519572108984, 0.0006447574123740196, 0.000434921239502728, 0.00012139035970903933, 0.0006722631515003741, 0.0010885755764320493, 0.038891106843948364]]]}\n",
       "    )\n",
       "    </script>"
      ],
      "text/plain": [
       "<circuitsvis.utils.render.RenderedHTML at 0x7f8ca07bebf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def hook_fn_queries(\n",
    "    q_input: Float[Tensor, \"batch seq n_heads d_model\"], \n",
    "    hook: HookPoint,\n",
    "    head: Tuple[int, int] = NEG_NMH,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    model: HookedTransformer = model,\n",
    "    project_in_S_dir: bool = True,\n",
    "    par: bool = False,\n",
    "):\n",
    "    unembed_IO = model.W_U.T[ioi_dataset.io_tokenIDs] # (batch, d_model)\n",
    "    unembed_S = model.W_U.T[ioi_dataset.s_tokenIDs] # (batch, d_model)\n",
    "\n",
    "    proj_dirs = [unembed_IO, unembed_S] if project_in_S_dir else [unembed_IO]\n",
    "    \n",
    "    q_slice = q_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], head[1]]\n",
    "    assert q_slice.shape == unembed_IO.shape\n",
    "    q_input_par, q_input_perp = project(q_slice, proj_dirs)\n",
    "\n",
    "    q_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], head[1]] = (q_input_par if par else q_input_perp)\n",
    "\n",
    "    return q_input\n",
    "\n",
    "\n",
    "model.reset_hooks()\n",
    "t.cuda.empty_cache()\n",
    "\n",
    "model.add_hook(utils.get_act_name(\"q_input\", NEG_NMH[0]), hook_fn_queries)\n",
    "\n",
    "logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"pattern\"))\n",
    "\n",
    "cv.attention.attention_patterns(\n",
    "    attention = cache[\"pattern\", NEG_NMH[0]][0, [NEG_NMH[1]]],\n",
    "    tokens = model.to_str_tokens(ioi_dataset.toks[0]),\n",
    "    # attention_head_names = [\"10.7\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff [qk] = 1.858\n",
      "Diff [q ] = 2.098\n",
      "Diff [ k] = 3.099\n",
      "Diff [  ] = 3.254\n"
     ]
    }
   ],
   "source": [
    "def hook_fn_keys(\n",
    "    k_input: Float[Tensor, \"batch seq n_heads d_model\"], \n",
    "    hook: HookPoint,\n",
    "    head: Tuple[int, int] = NEG_NMH,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "    ioi_cache: ActivationCache = ioi_cache,\n",
    "    model: HookedTransformer = model,\n",
    "    project_in_S_dir: bool = True,\n",
    "    par: bool = True,\n",
    "):\n",
    "    N = len(ioi_dataset)\n",
    "    mlp0_dir_IO = ioi_cache[\"mlp_out\", 0][range(N), ioi_dataset.word_idx[\"IO\"]] # (batch, d_model)\n",
    "    mlp0_dir_S = ioi_cache[\"mlp_out\", 0][range(N), ioi_dataset.word_idx[\"S1\"]] # (batch, d_model)\n",
    "\n",
    "    k_input_IO = k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"], head[1]]\n",
    "    k_input_S = k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"], head[1]]\n",
    "\n",
    "    assert k_input_IO.shape == mlp0_dir_IO.shape\n",
    "    k_input_IO_par, k_input_IO_perp = project(k_input_IO, mlp0_dir_IO)\n",
    "    k_input_S_par, k_input_S_perp = project(k_input_S, mlp0_dir_S)\n",
    "\n",
    "    k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"], head[1]] = (k_input_IO_par if par else k_input_IO_perp)\n",
    "    k_input[range(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"], head[1]] = (k_input_S_par if par else k_input_S_perp)\n",
    "\n",
    "    return k_input\n",
    "\n",
    "\n",
    "q_hook = (utils.get_act_name(\"q_normalized_input\", NEG_NMH[0]), hook_fn_queries)\n",
    "k_hook = (utils.get_act_name(\"k_normalized_input\", NEG_NMH[0]), hook_fn_keys)\n",
    "\n",
    "def test_model(model: HookedTransformer, show_too: bool = False):\n",
    "\n",
    "    for use_q, use_k in itertools.product([True, False], [True, False]):\n",
    "        model.reset_hooks()\n",
    "        if use_q: model.add_hook(*q_hook)\n",
    "        if use_k: model.add_hook(*k_hook)\n",
    "        desc = f\"{'q' if use_q else ' '}{'k' if use_k else ' '}\"\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "        logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"attn_scores\"))\n",
    "        attn_scores = cache[\"attn_scores\", NEG_NMH[0]][:, NEG_NMH[1]]\n",
    "\n",
    "        attn_scores_to_IO = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]]\n",
    "        attn_scores_to_S = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"S1\"]]\n",
    "\n",
    "        print(f\"Diff [{desc}] = {attn_scores_to_IO.mean() - attn_scores_to_S.mean():.3f}\")\n",
    "\n",
    "        if show_too:\n",
    "            labels = [f\"{x}_{i}\" for (i, x) in enumerate(model.to_str_tokens(ioi_dataset.toks[0]))]\n",
    "            imshow(\n",
    "                cache[\"attn_scores\", NEG_NMH[0]][0, NEG_NMH[1]],\n",
    "                x = labels,\n",
    "                y = labels,\n",
    "                labels = {\"x\": \"Key\", \"y\": \"Query\"},\n",
    "                height = 800,\n",
    "            )\n",
    "\n",
    "# cv.attention.attention_patterns(\n",
    "#     attention = cache[\"pattern\", NEG_NMH[0]][0, [NEG_NMH[1]]],\n",
    "#     tokens = model.to_str_tokens(ioi_dataset.toks[0]),\n",
    "#     attention_head_names = [\"10.7\"]\n",
    "# )\n",
    "# labels = [f\"{x}_{i}\" for (i, x) in enumerate(model.to_str_tokens(ioi_dataset.toks[0]))]\n",
    "\n",
    "\n",
    "test_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hook_fn_patch_wpos_MLP0(\n",
    "    resid_pre: Float[Tensor, \"batch seq d_model\"],\n",
    "    hook: HookPoint,\n",
    "    add: bool,\n",
    "    model: HookedTransformer = model,\n",
    "    permute: bool = False,\n",
    "    ioi_dataset: IOIDataset = ioi_dataset,\n",
    "):\n",
    "    seq_len = resid_pre.shape[1]\n",
    "    assert model.W_pos.shape[-1] == model.cfg.d_model\n",
    "    W_pos = model.W_pos[:seq_len]\n",
    "\n",
    "    if permute:\n",
    "        io_posses = W_pos[ioi_dataset.word_idx[\"IO\"]]\n",
    "        s_posses = W_pos[ioi_dataset.word_idx[\"S1\"]]\n",
    "\n",
    "        sign = 1.0 if add else -1.0\n",
    "\n",
    "        shape1 = resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"]].shape\n",
    "        shape2 = s_posses.shape\n",
    "        assert shape1==shape2\n",
    "\n",
    "        resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"IO\"]] += sign*(s_posses - io_posses)\n",
    "        resid_pre[torch.arange(len(ioi_dataset)), ioi_dataset.word_idx[\"S1\"]] += sign*(io_posses - s_posses)\n",
    "\n",
    "        return resid_pre\n",
    "\n",
    "    else:\n",
    "        if add:\n",
    "            return resid_pre + W_pos\n",
    "        else:\n",
    "            return resid_pre - W_pos\n",
    "\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "model.add_hook(utils.get_act_name(\"resid_pre\", 0), partial(hook_fn_patch_wpos_MLP0, add=True, permute=True), is_permanent=True)\n",
    "\n",
    "logits, mlp_positional_signals_flipped_cache = model.run_with_cache(ioi_dataset.toks)\n",
    "\n",
    "# model.add_hook(utils.get_act_name(\"resid_pre\", 1), partial(hook_fn_patch_wpos_MLP0, add=False, permute=True), is_permanent=True)\n",
    "\n",
    "# test_model(model, show_too=False)\n",
    "# model.reset_hooks(including_permanent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diff [qk] = 1.570\n",
    "# Diff [q ] = 1.711\n",
    "# Diff [ k] = 2.791\n",
    "# Diff [  ] = 2.911"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Path patching"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Path patch from MLP0 -> keyside 10.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->BAB, BAB->ABB\")\n",
    "# flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->BAA, BAB->ABA\")\n",
    "_, flipped_cache = model.run_with_cache(flipped_dataset.toks, names_filter=lambda name: name.endswith(\"mlp_out\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diff [qP] = 2.096\n",
      "Diff [q ] = 2.098\n",
      "Diff [ P] = 3.256\n",
      "Diff [  ] = 3.254\n"
     ]
    }
   ],
   "source": [
    "def patching_metric(cache: ActivationCache, print_std: bool = False) -> float:\n",
    "    attn_scores = cache[\"attn_scores\", 10][:, 7]\n",
    "    attn_scores_to_IO = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]]\n",
    "    attn_scores_to_S = attn_scores[range(len(ioi_dataset)), ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"S1\"]]\n",
    "    answer = attn_scores_to_IO - attn_scores_to_S \n",
    "    if print_std:\n",
    "        print(\"Std of mean:\", answer.std().item() / len(ioi_dataset))\n",
    "    return answer.mean().item()\n",
    "\n",
    "def test_model_PP(model: HookedTransformer):\n",
    "\n",
    "    for use_q, use_PP in itertools.product([True, False], [True, False]):\n",
    "        model.reset_hooks(including_permanent=True)\n",
    "        if use_q:\n",
    "            model.add_hook(*q_hook, is_permanent=True)\n",
    "        desc = f\"{'q' if use_q else ' '}{'P' if use_PP else ' '}\"\n",
    "\n",
    "        t.cuda.empty_cache()\n",
    "\n",
    "        if use_PP:\n",
    "            diff = path_patch(\n",
    "                model = model,\n",
    "                patching_metric = patching_metric,\n",
    "                apply_metric_to_cache = True,\n",
    "                orig_input = ioi_dataset.toks,\n",
    "                # new_input = flipped_dataset.toks,\n",
    "                orig_cache = ioi_cache,\n",
    "                new_cache = mlp_positional_signals_flipped_cache,\n",
    "                direct_includes_mlps = True,\n",
    "                sender_nodes = Node(\"mlp_out\", layer=0),\n",
    "                receiver_nodes = [Node(\"v\", layer=9, head=9), Node(\"v\", layer=9, head=6)],\n",
    "                # receiver_nodes = Node(\"k\", layer=10, head=7),\n",
    "            )\n",
    "        else:\n",
    "            # TODO - sanity check, do this with path_patch instead\n",
    "            logits, cache = model.run_with_cache(ioi_dataset.toks, names_filter=lambda name: name.endswith(\"attn_scores\"))\n",
    "            diff = patching_metric(cache)\n",
    "\n",
    "        print(f\"Diff [{desc}] = {diff:.3f}\")\n",
    "\n",
    "\n",
    "model.reset_hooks(including_permanent=True)\n",
    "test_model_PP(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->CBB, BAB->BCB\")\n",
    "flipped_dataset = ioi_dataset.gen_flipped_prompts(\"ABB->CDD, BAB->DCD\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "ioi_dataset.sentences[0], flipped_dataset.sentences[0]\n",
    "_, flipped_cache = model.run_with_cache(flipped_dataset.toks, names_filter=lambda name: (name.endswith(\"k\") and \".10.\" in name) or name.endswith(\"z\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Act patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[q ABB->CBB, BAB->BCB]: 0.063\n",
      "[  ABB->CBB, BAB->BCB]: -0.514\n",
      "[q ABB->ACC, BAB->CAC]: 2.202\n",
      "[  ABB->ACC, BAB->CAC]: 4.046\n"
     ]
    }
   ],
   "source": [
    "def test_act_patch(flip_string: str, add_q: bool = False):\n",
    "\n",
    "    model.reset_hooks(including_permanent=True)\n",
    "    if add_q:\n",
    "        model.add_hook(*q_hook, is_permanent=True)\n",
    "\n",
    "    flipped_dataset = ioi_dataset.gen_flipped_prompts(flip_string)\n",
    "    _, flipped_cache = model.run_with_cache(flipped_dataset.toks, names_filter=lambda name: (name.endswith(\"k\") and \".10.\" in name) or name.endswith(\"z\"))\n",
    "\n",
    "    return act_patch(\n",
    "        model = model,\n",
    "        orig_input = ioi_dataset.toks,\n",
    "        patching_nodes = Node(\"k\", layer=10, head=7),\n",
    "        new_cache = flipped_cache,\n",
    "        apply_metric_to_cache = True,\n",
    "        patching_metric = patching_metric,\n",
    "    )\n",
    "\n",
    "for flip_string in [\n",
    "    \"ABB->CBB, BAB->BCB\",\n",
    "    # \"ABB->CDD, BAB->DCD\",\n",
    "    \"ABB->ACC, BAB->CAC\",\n",
    "]:\n",
    "    for add_q in [True, False]:\n",
    "        print(f\"[{'q' if add_q else ' '} {flip_string}]: {test_act_patch(flip_string, add_q=add_q):.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.11 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d4d1e4263499bec80672ea0156c357c1ee493ec2b1c70f0acce89fc37c4a6abe"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
