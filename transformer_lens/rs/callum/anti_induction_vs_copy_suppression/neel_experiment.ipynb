{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer_lens.cautils.notebook import *\n",
    "\n",
    "t.set_grad_enabled(False)\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "gpt2 = HookedTransformer.from_pretrained(\n",
    "    \"gpt2-small\",\n",
    "    center_unembed=True,\n",
    "    center_writing_weights=True,\n",
    "    fold_ln=True,\n",
    "    device=device,\n",
    "    # refactor_factored_attn_matrices=True,\n",
    ")\n",
    "gpt2.set_use_attn_result(False)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_copy_suppression_scores_ioi(model: HookedTransformer, N: int):\n",
    "\n",
    "    all_results = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device, dtype=t.float)\n",
    "\n",
    "    ioi_dataset, ioi_cache = generate_data_and_caches(N, model, seed=42, prepend_bos=True, only_ioi=True, symmetric=True)\n",
    "\n",
    "    io_unembeddings = model.W_U.T[ioi_dataset.io_tokenIDs] # (batch, d_model)\n",
    "\n",
    "    scale = ioi_cache[\"scale\"] # (batch, seq, 1)\n",
    "    scale = scale[range(N), ioi_dataset.word_idx[\"end\"]] # (batch, 1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        v = ioi_cache[\"v\", layer] # (batch, seq, n_heads, d_head)\n",
    "\n",
    "        v_io = v[range(N), ioi_dataset.word_idx[\"IO\"]] # (batch, n_heads, d_head)\n",
    "\n",
    "        # Get result (before attn patterns)\n",
    "        result_io = einops.einsum(\n",
    "            v_io, model.W_O[layer],\n",
    "            \"batch n_heads d_head, n_heads d_head d_model -> batch n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Get result moved to `end` token (after attn patterns)\n",
    "        patterns = ioi_cache[\"pattern\", layer] # (batch, n_heads, seqQ, seqK)\n",
    "        patterns_end_to_io = patterns[range(N), :, ioi_dataset.word_idx[\"end\"], ioi_dataset.word_idx[\"IO\"]] # (batch, n_heads)\n",
    "        result_io_to_end = einops.einsum(\n",
    "            result_io, patterns_end_to_io,\n",
    "            \"batch n_heads d_model, batch n_heads -> batch n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Finally, get attribution (which includes effect of layernorm)\n",
    "        dla = einops.einsum(\n",
    "            result_io_to_end, io_unembeddings,\n",
    "            \"batch n_heads d_model, batch d_model -> batch n_heads\"\n",
    "        ) / scale\n",
    "        dla = einops.reduce(dla, \"batch n_heads -> n_heads\", \"mean\")\n",
    "        \n",
    "        all_results[layer] = dla\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_anti_induction_scores(model: HookedTransformer, N: int, seq_len: int = 30):\n",
    "\n",
    "    tokens_to_repeat = t.randint(0, model.cfg.d_vocab, (N, seq_len), device=device)\n",
    "    bos_tokens = t.full((N, 1), model.tokenizer.bos_token_id, device=device, dtype=t.long)\n",
    "    tokens = t.concat([bos_tokens, tokens_to_repeat, tokens_to_repeat], dim=1)\n",
    "    assert tokens.shape == (N, 2*seq_len+1)\n",
    "        \n",
    "    all_results = t.zeros((model.cfg.n_layers, model.cfg.n_heads), device=device, dtype=t.float)\n",
    "\n",
    "    _, cache = model.run_with_cache(tokens, return_type = None)\n",
    "\n",
    "    rep_unembeddings = model.W_U.T[tokens_to_repeat[:, 1:]] # (batch, rep_seq_pos, d_model)\n",
    "\n",
    "    batch_indices = einops.repeat(t.arange(N, device=device), \"batch -> batch seq\", seq=seq_len-1)\n",
    "    dest_indices = einops.repeat(t.arange(seq_len+1, 2*seq_len, device=device), \"seq -> batch seq\", batch=N)\n",
    "    src_indices = einops.repeat(t.arange(2, seq_len+1, device=device), \"seq -> batch seq\", batch=N)\n",
    "\n",
    "    scale = cache[\"scale\"] # (batch, seq, 1)\n",
    "    scale = scale[batch_indices, dest_indices] # (batch, rep_seq_pos, 1)\n",
    "\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        v = cache[\"v\", layer] # (batch, seq, n_heads, d_head)\n",
    "\n",
    "        v_io = v[batch_indices, src_indices] # (batch, rep_seq_pos, n_heads, d_head)\n",
    "\n",
    "        # Get result (before attn patterns)\n",
    "        result_io = einops.einsum(\n",
    "            v_io, model.W_O[layer],\n",
    "            \"batch rep_seq_pos n_heads d_head, n_heads d_head d_model -> batch rep_seq_pos n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Get result moved to dest tokens (after attn patterns)\n",
    "        patterns = cache[\"pattern\", layer] # (batch, n_heads, seqQ, seqK)\n",
    "        patterns_end_to_io = patterns[batch_indices, :, dest_indices, src_indices] # (batch, rep_seq_pos, n_heads)\n",
    "        result_io_to_end = einops.einsum(\n",
    "            result_io, patterns_end_to_io,\n",
    "            \"batch rep_seq_pos n_heads d_model, batch rep_seq_pos n_heads -> batch rep_seq_pos n_heads d_model\"\n",
    "        )\n",
    "\n",
    "        # Finally, get attribution (which includes effect of layernorm)\n",
    "        dla = einops.einsum(\n",
    "            result_io_to_end, rep_unembeddings,\n",
    "            \"batch rep_seq_pos n_heads d_model, batch rep_seq_pos d_model -> batch rep_seq_pos n_heads\"\n",
    "        ) / scale\n",
    "        dla = einops.reduce(dla, \"batch rep_seq_pos n_heads -> n_heads\", \"mean\")\n",
    "        \n",
    "        all_results[layer] = dla\n",
    "\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model_scores(model_name: str, N: int, plot: bool = False):\n",
    "\n",
    "    t.cuda.empty_cache()\n",
    "\n",
    "    model = HookedTransformer.from_pretrained(\n",
    "        model_name,\n",
    "        center_unembed=True,\n",
    "        center_writing_weights=True,\n",
    "        fold_ln=True,\n",
    "        device=device\n",
    "        # refactor_factored_attn_matrices=True,\n",
    "    )\n",
    "    model.set_use_attn_result(False)\n",
    "\n",
    "    copy_suppression_scores_ioi = get_copy_suppression_scores_ioi(model, N)\n",
    "    neg_copy_suppression_scores_ioi = copy_suppression_scores_ioi * (copy_suppression_scores_ioi < 0)\n",
    "\n",
    "    anti_induction_scores = get_anti_induction_scores(model, N)\n",
    "    neg_anti_induction_scores = anti_induction_scores * (anti_induction_scores < 0)\n",
    "\n",
    "    model_scores = t.stack([copy_suppression_scores_ioi, anti_induction_scores])\n",
    "\n",
    "    RESULTS_DIR = Path(\"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/anti_induction_vs_copy_suppression/model_results\")\n",
    "\n",
    "    with open(RESULTS_DIR / f\"scores_{model_name}.pkl\", \"wb\") as f:\n",
    "        pickle.dump(model_scores, f)\n",
    "\n",
    "    if plot:\n",
    "        imshow(model_scores, title=model_name, facet_col=0, text_auto=\".1f\", width=1000, height=500, static=True, facet_labels=[\"IOI Copy Suppression DLA\", \"Anti-Induction DLA\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SMALL_MODEL_NAMES = [\n",
    "    \"distillgpt2\",\n",
    "    \"gpt2-small\",\n",
    "    *[f\"stanford-gpt2-small-{i}\" for i in \"abcde\"],\n",
    "    *[f\"pythia-{n}m\" for n in [70, 160]],\n",
    "    *[f\"pythia-{n}m-deduped\" for n in [70, 160]],\n",
    "    *[f\"solu-{n}l\" for n in [4, 6, 8, 10]],\n",
    "    *[f\"solu-{n}l-pile\" for n in [4, 6, 8, 10]],\n",
    "    \"gelu-4l\",\n",
    "    \"gpt-neo-125m\",\n",
    "    \"opt-125m\",\n",
    "]\n",
    "MEDIUM_MODEL_NAMES = [\n",
    "    \"gpt-neo-125m\",\n",
    "    \"gpt2-medium\",\n",
    "    *[f\"stanford-gpt2-medium-{i}\" for i in \"abcde\"],\n",
    "    *[f\"pythia-{n}m\" for n in [410]],\n",
    "    *[f\"pythia-{n}m-deduped\" for n in [410]],\n",
    "    \"solu-12l\",\n",
    "    \"gpt2-large\",\n",
    "]\n",
    "BIG_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [1.4, 2.8]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [1.4, 2.8]],\n",
    "    \"gpt2-xl\",\n",
    "    \"gpt-neo-2.7B\",\n",
    "    \"opt-1.3b\",\n",
    "    \"opt-2.7b\",\n",
    "]\n",
    "GIANT_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [6.9]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [6.9]],\n",
    "    \"gpt-j-6B\",\n",
    "    \"opt-6.7b\",\n",
    "]\n",
    "BROBDINGNAGIAN_MODEL_NAMES = [\n",
    "    *[f\"pythia-{n}b\" for n in [12]],\n",
    "    *[f\"pythia-{n}b-deduped\" for n in [12]],\n",
    "    \"gpt-neox-20b\",\n",
    "    \"opt-13b\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in SMALL_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in MEDIUM_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in BIG_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model_name in GIANT_MODEL_NAMES:\n",
    "    t0 = time.time()\n",
    "    save_model_scores(model_name, N=100, plot=False)\n",
    "    print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for model_name in BROBDINGNAGIAN_MODEL_NAMES:\n",
    "#     t0 = time.time()\n",
    "#     save_model_scores(model_name, N=100, plot=False)\n",
    "#     print(f\"Finished {model_name} in {time.time() - t0:.2f}s\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'scores_gpt-j-6B.pkl',\n",
       " 'scores_gpt-neo-2.7B.pkl',\n",
       " 'scores_pythia-6.9b-deduped.pkl',\n",
       " 'scores_pythia-6.9b.pkl'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new = \"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/anti_induction_vs_copy_suppression/model_results\"\n",
    "old = \"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/streamlit/anti_induction_vs_copy_suppression/model_results\"\n",
    "\n",
    "new = list(map(lambda x: x.name, Path(new).iterdir()))\n",
    "old = list(map(lambda x: x.name, Path(old).iterdir()))\n",
    "\n",
    "set(new) - set(old)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "set()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(old) - set(new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def plot_all_results():\n",
    "    results_copy_suppression_ioi = []\n",
    "    results_anti_induction = []\n",
    "    model_names = []\n",
    "    head_names = []\n",
    "\n",
    "    RESULTS_DIR = Path(\"/home/ubuntu/Transformerlens/transformer_lens/rs/callum/anti_induction_vs_copy_suppression/model_results\")\n",
    "\n",
    "    for file in RESULTS_DIR.iterdir():\n",
    "        with open(file, \"rb\") as f:\n",
    "            model_scores: Tensor = pickle.load(f)\n",
    "\n",
    "            for layer in range(model_scores.size(1)):\n",
    "                for head in range(model_scores.size(2)):\n",
    "                    results_copy_suppression_ioi.append(model_scores[0, layer, head].item())\n",
    "                    results_anti_induction.append(model_scores[1, layer, head].item())\n",
    "                    model_names.append(file.stem.replace(\"scores_\", \"\"))\n",
    "                    head_names.append(f\"{layer}.{head}\")\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        \"results_copy_suppression_ioi\": results_copy_suppression_ioi,\n",
    "        \"results_anti_induction\": results_anti_induction,\n",
    "        \"model_names\": model_names,\n",
    "        \"head_names\": head_names\n",
    "    })\n",
    "\n",
    "    fig = px.scatter(\n",
    "        df,\n",
    "        x=\"results_copy_suppression_ioi\", y=\"results_anti_induction\", color='model_names', hover_data=[\"model_names\", \"head_names\"],\n",
    "        width=1200,\n",
    "        height=800,\n",
    "        title=\"Anti-Induction Scores (repeated random tokens) vs Copy Suppression Scores (IOI)\",\n",
    "        labels={\"results_copy_suppression_ioi\": \"Copy Suppression\", \"results_anti_induction\": \"Anti-Induction\"}\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "plot_all_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
